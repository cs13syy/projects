{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = True\n",
    "evaluation_verbose = False\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://capstone-01/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://capstone-01/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해시 함수를 가진 모듈 hashlib\n",
    "import hashlib\n",
    "def hashstr(s, nr_bins):\n",
    "    return int(hashlib.md5(s.encode('utf8')).hexdigest(), 16)%(nr_bins-1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_time_to_unix_epoch 함수 생성\n",
    "def date_time_to_unix_epoch(date_time):\n",
    "    return int(time.mktime(date_time.timetuple()))\n",
    "\n",
    "# timetuple() : date 객체의 값을 time.struct_time 시퀀스 객체에 할당. 해당되는 정보가 없는 시,분,초는 '0'으로 초기화\n",
    "# d = datetime.date.today()\n",
    "# d.timetuple() -> time.struct_time(tm_year=2011, tm_mon=11...)\n",
    "# time.mktime() : 인자로 받아서 time()과 같은 누적된 초를 반환  \n",
    "# t = (2009, 2, 17, 17, 3, 38, 1, 48, 0)\n",
    "# time.mktime(t) : 1234915418.000000\n",
    "# asctime(localtime(secs)): Tue Feb 17 17:03:38 2009\n",
    "  \n",
    "# date_tiem_to_unix_epoch_treated 함수 생성\n",
    "# try 블록 수행 중 오류가 발생하면 except 블록이 수행된다\n",
    "# 입력 변수에 대해 date_time_to_unix_epoch 함수값이 나오지 않으면 오류 발생시킨다\n",
    "def date_time_to_unix_epoch_treated(dt):\n",
    "    if dt != None:\n",
    "        try:\n",
    "            epoch = date_time_to_unix_epoch(dt)\n",
    "            return epoch\n",
    "        except Exception as e:\n",
    "            print(\"Error processing dt={}\".format(dt), e)\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch를 integer로 반환\n",
    "timestamp_null_to_zero_int_udf = F.udf(lambda x: date_time_to_unix_epoch_treated(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_null_to_minus_one_udf -> int가 null값이면 -1을 반환하는 함수\n",
    "# 아래 3개 함수 -> 각각 int, float, str 타입인 리스트 형태 데이터가 null 값이면 빈 리스트 반환하는 함수\n",
    "\n",
    "INT_DEFAULT_NULL_VALUE = -1\n",
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else INT_DEFAULT_NULL_VALUE, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_day_from_timestamp(ts):\n",
    "    return int(ts / 1000 / 60 / 60 / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: truncate_day_from_timestamp(ts), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_country_udf는 geo_location에서 앞 두 글자(국가명)만 가져오는 함수\n",
    "\n",
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_country_udf는 geo_location에서 앞 다섯 글자(국가명+주명)만 가져오는 함수\n",
    "\n",
    "extract_country_state_udf = F.udf(lambda geo: geo.strip()[:5] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_len_udf는 input값의 길이를 반환하는 함수\n",
    "\n",
    "list_len_udf = F.udf(lambda x: len(x) if x != None else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_odd_timestamp는 timestamp 원데이터 값을 년월일 형식으로 변환해주는 함수\n",
    "# kaggle data explanation : If you wish to recover the actual epoch time of the visit, add 1465876799998 to the timestamp.\n",
    "\n",
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 6, 14, 4, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_odd_timestamp(61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load files : Loading UTC/BST for each country and US / CA states (local time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_codes_utc_dst_tz_delta.csv는 다음 2개 컬럼을 가짐 -> country_code, utc_dst_time_offset_cleaned\n",
    "\n",
    "country_utc_dst_df = pd.read_csv('country_codes_utc_dst_tz_delta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_utc_dst_dict = dict(zip(country_utc_dst_df['country_code'].tolist(), country_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "countries_utc_dst_broad = sc.broadcast(countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_states_abbrev_bst.csv는 다음 2개 컬럼을 가짐 -> state_abb, utc_dst_time_offset_cleaned\n",
    "\n",
    "us_states_utc_dst_df = pd.read_csv('us_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_utc_dst_dict = dict(zip(us_states_utc_dst_df['state_abb'].tolist(), us_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "us_states_utc_dst_broad = sc.broadcast(us_states_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca_states_abbrev_bst.csv는 다음 2개 컬럼을 가짐 -> state_abb, utc_dst_time_offset_cleaned\n",
    "\n",
    "ca_states_utc_dst_df = pd.read_csv('ca_states_abbrev_bst.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_countries_utc_dst_dict = dict(zip(ca_states_utc_dst_df['state_abb'].tolist(), ca_states_utc_dst_df['utc_dst_time_offset_cleaned'].tolist()))\n",
    "ca_countries_utc_dst_broad = sc.broadcast(ca_countries_utc_dst_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "                .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                .withColumn('event_country_state', extract_country_state_udf('geo_location_event')) \\\n",
    "                .alias('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+\n",
      "|display_id|    uuid_event|document_id_event|timestamp_event|platform_event|geo_location_event|dummyEvents|day_event|event_country|event_country_state|\n",
      "+----------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+\n",
      "|         1|cb8c55702adb93|           379743|             61|             3|         US>SC>519|          1|        0|           US|              US>SC|\n",
      "|         2|79a85fa78311b9|          1794259|             81|             2|         US>CA>807|          1|        0|           US|              US>CA|\n",
      "|         3|822932ce3d8757|          1179111|            182|             2|         US>MI>505|          1|        0|           US|              US>MI|\n",
      "+----------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "# Google Storage에 저장된 page_views 파일 로드\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(\"gs://upload-bigquery180927/page_views.csv\") \\\n",
    "                .withColumn('day_pv', truncate_day_from_timestamp_udf('timestamp_pv')) \\\n",
    "                .alias('page_views')             \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_users_df  = spark.sql('''\n",
    "                    SELECT uuid_pv, document_id_pv, max(timestamp_pv) as max_timestamp_pv, 1 as dummyPageView\n",
    "                    FROM page_views p \n",
    "                    GROUP BY uuid_pv, document_id_pv\n",
    "                    ''').alias('page_views_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_content_schema = StructType(\n",
    "                    [StructField(\"ad_id\", IntegerType(), True),\n",
    "                    StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                    StructField(\"campaign_id\", IntegerType(), True),\n",
    "                    StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining with Page Views to get traffic_source_pv\n",
    "# events, documents_meta를 조인 (key='document_id')\n",
    "# 위 데이터프레임과 page_views를 조인 (key=5개 컬럼의 데이터가 다 일치하는 데이터만 가져옴)\n",
    "events_joined_df = events_df.join(documents_meta_df \\\n",
    "                                  .withColumnRenamed('source_id', 'source_id_doc_event') \\\n",
    "                                  .withColumnRenamed('publisher_id', 'publisher_doc_event') \\\n",
    "                                  .withColumnRenamed('publish_time', 'publish_time_doc_event')\n",
    "                                  , on=F.col(\"document_id_event\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                            .join(page_views_df, \n",
    "                                           on=[F.col('uuid_event') == F.col('uuid_pv'),\n",
    "                                               F.col('document_id_event') == F.col('document_id_pv'),\n",
    "                                               F.col('platform_event') == F.col('platform_pv'),\n",
    "                                               F.col('geo_location_event') == F.col('geo_location_pv'),\n",
    "                                               F.col('day_event') == F.col('day_pv')],\n",
    "                                           how='left') \\\n",
    "                                    .alias('events').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_joined_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories').cache()\n",
    "    \n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "                                                 F.collect_list('confidence_level_cat').alias('confidence_level_cat_list')) \\\n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "                                            .alias('documents_categories_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics').cache()\n",
    "    \n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "                                                 F.collect_list('confidence_level_top').alias('confidence_level_top_list')) \\\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "                                            .alias('documents_topics_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\")  \\\n",
    "                .alias('documents_entities').cache()\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "                                            .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "                                                 F.collect_list('confidence_level_ent').alias('confidence_level_ent_list')) \\\n",
    "                                            .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "                                            .alias('documents_entities_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_train_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True),                    \n",
    "                    StructField(\"clicked\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "                .withColumn('dummyClicksTrain', F.lit(1)).alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_train_joined_df = clicks_train_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- display_id: integer (nullable = true)\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- clicked: integer (nullable = true)\n",
      " |-- dummyClicksTrain: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clicks_train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- document_id_promo: integer (nullable = true)\n",
      " |-- campaign_id: integer (nullable = true)\n",
      " |-- advertiser_id: integer (nullable = true)\n",
      " |-- dummyPromotedContent: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "promoted_content_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document_id_doc: integer (nullable = true)\n",
      " |-- source_id: integer (nullable = true)\n",
      " |-- publisher_id: integer (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- dummyDocumentsMeta: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_meta_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- display_id: integer (nullable = true)\n",
      " |-- uuid_event: string (nullable = true)\n",
      " |-- document_id_event: integer (nullable = true)\n",
      " |-- timestamp_event: integer (nullable = true)\n",
      " |-- platform_event: integer (nullable = true)\n",
      " |-- geo_location_event: string (nullable = true)\n",
      " |-- dummyEvents: integer (nullable = false)\n",
      " |-- day_event: integer (nullable = true)\n",
      " |-- event_country: string (nullable = true)\n",
      " |-- event_country_state: string (nullable = true)\n",
      " |-- document_id_doc: integer (nullable = true)\n",
      " |-- source_id_doc_event: integer (nullable = true)\n",
      " |-- publisher_doc_event: integer (nullable = true)\n",
      " |-- publish_time_doc_event: timestamp (nullable = true)\n",
      " |-- dummyDocumentsMeta: integer (nullable = true)\n",
      " |-- uuid_pv: string (nullable = true)\n",
      " |-- document_id_pv: integer (nullable = true)\n",
      " |-- timestamp_pv: integer (nullable = true)\n",
      " |-- platform_pv: integer (nullable = true)\n",
      " |-- geo_location_pv: string (nullable = true)\n",
      " |-- traffic_source_pv: integer (nullable = true)\n",
      " |-- day_pv: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- display_id: integer (nullable = true)\n",
      " |-- ad_id: integer (nullable = true)\n",
      " |-- clicked: integer (nullable = true)\n",
      " |-- dummyClicksTrain: integer (nullable = false)\n",
      " |-- document_id_promo: integer (nullable = true)\n",
      " |-- campaign_id: integer (nullable = true)\n",
      " |-- advertiser_id: integer (nullable = true)\n",
      " |-- dummyPromotedContent: integer (nullable = true)\n",
      " |-- document_id_doc: integer (nullable = true)\n",
      " |-- source_id: integer (nullable = true)\n",
      " |-- publisher_id: integer (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- dummyDocumentsMeta: integer (nullable = true)\n",
      " |-- uuid_event: string (nullable = true)\n",
      " |-- document_id_event: integer (nullable = true)\n",
      " |-- timestamp_event: integer (nullable = true)\n",
      " |-- platform_event: integer (nullable = true)\n",
      " |-- geo_location_event: string (nullable = true)\n",
      " |-- dummyEvents: integer (nullable = true)\n",
      " |-- day_event: integer (nullable = true)\n",
      " |-- event_country: string (nullable = true)\n",
      " |-- event_country_state: string (nullable = true)\n",
      " |-- document_id_doc: integer (nullable = true)\n",
      " |-- source_id_doc_event: integer (nullable = true)\n",
      " |-- publisher_doc_event: integer (nullable = true)\n",
      " |-- publish_time_doc_event: timestamp (nullable = true)\n",
      " |-- dummyDocumentsMeta: integer (nullable = true)\n",
      " |-- uuid_pv: string (nullable = true)\n",
      " |-- document_id_pv: integer (nullable = true)\n",
      " |-- timestamp_pv: integer (nullable = true)\n",
      " |-- platform_pv: integer (nullable = true)\n",
      " |-- geo_location_pv: string (nullable = true)\n",
      " |-- traffic_source_pv: integer (nullable = true)\n",
      " |-- day_pv: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clicks_train_joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+----------------+-----------------+-----------+-------------+--------------------+---------------+---------+------------+-------------------+------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+---------------+-------------------+-------------------+----------------------+------------------+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "|display_id| ad_id|clicked|dummyClicksTrain|document_id_promo|campaign_id|advertiser_id|dummyPromotedContent|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|    uuid_event|document_id_event|timestamp_event|platform_event|geo_location_event|dummyEvents|day_event|event_country|event_country_state|document_id_doc|source_id_doc_event|publisher_doc_event|publish_time_doc_event|dummyDocumentsMeta|       uuid_pv|document_id_pv|timestamp_pv|platform_pv|geo_location_pv|traffic_source_pv|day_pv|\n",
      "+----------+------+-------+----------------+-----------------+-----------+-------------+--------------------+---------------+---------+------------+-------------------+------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+---------------+-------------------+-------------------+----------------------+------------------+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "|       148| 89351|      1|               1|           990613|       7617|         2181|                   1|         990613|     9457|        null|2015-12-09 00:00:00|                 1|9adce6a5363308|          1205772|          11202|             2|         US>LA>612|          1|        0|           US|              US>LA|        1205772|               9135|               1042|   2016-03-29 09:00:00|                 1|9adce6a5363308|       1205772|       11202|          2|      US>LA>612|                1|     0|\n",
      "|       148|152656|      0|               1|          1086755|      10511|         2151|                   1|        1086755|     7654|        null|2015-11-17 00:00:00|                 1|9adce6a5363308|          1205772|          11202|             2|         US>LA>612|          1|        0|           US|              US>LA|        1205772|               9135|               1042|   2016-03-29 09:00:00|                 1|9adce6a5363308|       1205772|       11202|          2|      US>LA>612|                1|     0|\n",
      "|       148|152140|      0|               1|          1060089|      19032|         1593|                   1|        1060089|     7744|        1204|2016-01-23 01:00:00|                 1|9adce6a5363308|          1205772|          11202|             2|         US>LA>612|          1|        0|           US|              US>LA|        1205772|               9135|               1042|   2016-03-29 09:00:00|                 1|9adce6a5363308|       1205772|       11202|          2|      US>LA>612|                1|     0|\n",
      "+----------+------+-------+----------------+-----------------+-----------+-------------+--------------------+---------------+---------+------------+-------------------+------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+---------------+-------------------+-------------------+----------------------+------------------+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 52 ms, sys: 28 ms, total: 80 ms\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%time clicks_train_joined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'\n",
    "\n",
    "user_profiles_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+table_name) \\\n",
    "                    .withColumn('dummyUserProfiles', F.lit(1)).alias('user_profiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4961756"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profiles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Train/validation set | Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_set_df.count() =', 59761474)\n"
     ]
    }
   ],
   "source": [
    "if evaluation:       \n",
    "    validation_set_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "                    .alias('validation_set') \n",
    "            \n",
    "    validation_set_exported_df.select('display_id').distinct().createOrReplaceTempView(\"validation_display_ids\")\n",
    "    \n",
    "    \n",
    "    validation_set_df = spark.sql('''SELECT * FROM clicks_train_joined t \n",
    "             WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                           WHERE display_id = t.display_id)''').alias('clicks') \\\n",
    "                         .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .join(documents_categories_grouped_df \\\n",
    "                                   .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                   .alias('documents_event_categories_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_topics_grouped_df \\\n",
    "                                   .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                   .alias('documents_event_topics_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_entities_grouped_df \\\n",
    "                                   .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                   .alias('documents_event_entities_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                               how='left') \\\n",
    "                         .join(page_views_users_df, on=[F.col(\"clicks.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "                                                        F.col(\"clicks.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "                                                  how='left')\n",
    "    \n",
    "    #print(\"validation_set_df.count() =\", validation_set_df.count())\n",
    "        \n",
    "    #Added to validation set information about the event and the user for statistics of the error (avg ctr)\n",
    "    validation_set_ground_truth_df = validation_set_df.filter('clicked = 1') \\\n",
    "                                .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                                .withColumn('user_categories_count', list_len_udf('category_id_list')) \\\n",
    "                                .withColumn('user_topics_count', list_len_udf('topic_id_list')) \\\n",
    "                                .withColumn('user_entities_count', list_len_udf('entity_id_list')) \\\n",
    "                                .select('display_id','ad_id','platform_event', 'day_event', 'timestamp_event', \n",
    "                                        'geo_location_event', 'event_country', 'event_country_state', 'views',\n",
    "                                        'user_categories_count', 'user_topics_count', 'user_entities_count') \\\n",
    "                                .withColumnRenamed('ad_id','ad_id_gt') \\\n",
    "                                .withColumnRenamed('views','user_views_count') \\\n",
    "                                .cache()\n",
    "    #print(\"validation_set_ground_truth_df.count() =\", validation_set_ground_truth_df.count())\n",
    "    \n",
    "    train_set_df = spark.sql('''SELECT * FROM clicks_train_joined t \n",
    "                                 WHERE NOT EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                                               WHERE display_id = t.display_id)''').cache()\n",
    "    print(\"train_set_df.count() =\", train_set_df.count())\n",
    "    \n",
    "    #validation_display_ids_df.groupBy(\"day_event\").count().show()\n",
    "    \n",
    "else:\n",
    "    \n",
    "    clicks_test_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER + \"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)) \\\n",
    "                    .withColumn('clicked', F.lit(-999)) \\\n",
    "                    .alias('clicks_test')\n",
    "        \n",
    "        \n",
    "    test_set_df = clicks_test_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "                         .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .join(events_joined_df, on='display_id', how='left') \\\n",
    "                         .join(documents_categories_grouped_df \\\n",
    "                                   .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                   .alias('documents_event_categories_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_topics_grouped_df \\\n",
    "                                   .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                   .alias('documents_event_topics_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                               how='left') \\\n",
    "                         .join(documents_entities_grouped_df \\\n",
    "                                   .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                   .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                   .alias('documents_event_entities_grouped'), \n",
    "                               on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                               how='left') \\\n",
    "                         .join(page_views_users_df, on=[F.col(\"events.uuid_event\") == F.col(\"page_views_users.uuid_pv\"),\n",
    "                                                        F.col(\"promoted_content.document_id_promo\") == F.col(\"page_views_users.document_id_pv\")], \n",
    "                                                  how='left')\n",
    "\n",
    "    #print(\"test_set_df.count() =\",test_set_df.count())\n",
    "   \n",
    "    \n",
    "    train_set_df = clicks_train_joined_df.cache() \n",
    "    print(\"train_set_df.count() =\", train_set_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+----------------+-----------------+-----------+-------------+--------------------+---------------+---------+------------+-------------------+------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+---------------+-------------------+-------------------+----------------------+------------------+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "|display_id| ad_id|clicked|dummyClicksTrain|document_id_promo|campaign_id|advertiser_id|dummyPromotedContent|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|    uuid_event|document_id_event|timestamp_event|platform_event|geo_location_event|dummyEvents|day_event|event_country|event_country_state|document_id_doc|source_id_doc_event|publisher_doc_event|publish_time_doc_event|dummyDocumentsMeta|       uuid_pv|document_id_pv|timestamp_pv|platform_pv|geo_location_pv|traffic_source_pv|day_pv|\n",
      "+----------+------+-------+----------------+-----------------+-----------+-------------+--------------------+---------------+---------+------------+-------------------+------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+---------------+-------------------+-------------------+----------------------+------------------+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "|       148|152656|      0|               1|          1086755|      10511|         2151|                   1|        1086755|     7654|        null|2015-11-17 00:00:00|                 1|9adce6a5363308|          1205772|          11202|             2|         US>LA>612|          1|        0|           US|              US>LA|        1205772|               9135|               1042|   2016-03-29 09:00:00|                 1|9adce6a5363308|       1205772|       11202|          2|      US>LA>612|                1|     0|\n",
      "|       148|326768|      0|               1|          1379561|      23374|         2348|                   1|        1379561|     9419|        1144|2016-05-01 06:00:00|                 1|9adce6a5363308|          1205772|          11202|             2|         US>LA>612|          1|        0|           US|              US>LA|        1205772|               9135|               1042|   2016-03-29 09:00:00|                 1|9adce6a5363308|       1205772|       11202|          2|      US>LA>612|                1|     0|\n",
      "|       148| 89351|      1|               1|           990613|       7617|         2181|                   1|         990613|     9457|        null|2015-12-09 00:00:00|                 1|9adce6a5363308|          1205772|          11202|             2|         US>LA>612|          1|        0|           US|              US>LA|        1205772|               9135|               1042|   2016-03-29 09:00:00|                 1|9adce6a5363308|       1205772|       11202|          2|      US>LA>612|                1|     0|\n",
      "+----------+------+-------+----------------+-----------------+-----------+-------------+--------------------+---------------+---------+------------+-------------------+------------------+--------------+-----------------+---------------+--------------+------------------+-----------+---------+-------------+-------------------+---------------+-------------------+-------------------+----------------------+------------------+--------------+--------------+------------+-----------+---------------+-----------------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_set_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 확인하는 함수 (value가 없거나, value의 str 길이가 0일 때)\n",
    "def is_null(value):\n",
    "    return value == None or len(str(value).strip()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LESS_SPECIAL_CAT_VALUE = 'less'\n",
    "def get_category_field_values_counts(field, df, min_threshold=10):\n",
    "    category_counts = dict(list(filter(lambda x: not is_null(x[0]) and x[1] >= min_threshold, \n",
    "                                       df.select(field).groupBy(field).count().rdd.map(lambda x: (x[0], x[1])).collect())))\n",
    "    #Adding a special value to create a feature for values in this category that are less than min_threshold \n",
    "    category_counts[LESS_SPECIAL_CAT_VALUE] = -1\n",
    "    return category_counts\n",
    "  \n",
    "# get_category_field_values_counts() -> value가 10개 이상인 카테고리 개수 출력\n",
    "# spark 문법 : filter, rdd, rdd.map, collect\n",
    "# filter : filter()로 전달된 함수의 조건에 통과한 값만 리턴\n",
    "# rdd : 스파크의 기본적인 데이터 단위\n",
    "# rdd.map : rdd의 각 요소에 함수를 적용하고 결과 rdd를 리턴\n",
    "# collect : rdd의 모든 데이터 리턴\n",
    "# min_threshold : 10개 이상인 카테고리만 인정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building category values counters and indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# events의 geo_location에 기재된 국가 카테고리 개수 (value가 10개 이상인)\n",
    "event_country_values_counts = get_category_field_values_counts('event_country', events_df, min_threshold=10)\n",
    "len(event_country_values_counts)\n",
    "#All non-null categories: 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# events의 geo_location에 기재된 국가-주 카테고리 개수 (value가 10개 이상인)\n",
    "event_country_state_values_counts = get_category_field_values_counts('event_country_state', events_df, min_threshold=10)\n",
    "len(event_country_state_values_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# events의 geo_location 전체 개수 (value가 10개 이상인)\n",
    "event_geo_location_values_counts = get_category_field_values_counts('geo_location_event', events_df, min_threshold=10)\n",
    "len(event_geo_location_values_counts)\n",
    "#All non-null categories: 2988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52439"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents_entities의 entity_id 전체 개수 (value가 10개 이상인)\n",
    "doc_entity_id_values_counts = get_category_field_values_counts('entity_id', documents_entities_df, min_threshold=10)\n",
    "len(doc_entity_id_values_counts)\n",
    "#All non-null categories: 1326009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing average CTR by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentiles(df, field, quantiles_levels=None, max_error_rate=0.0):\n",
    "    if quantiles_levels == None:\n",
    "        quantiles_levels = np.arange(0.0, 1.1, 0.1).tolist() # 0.0, 0.1,..., 1.0\n",
    "    quantiles = df.approxQuantile(field, quantiles_levels, max_error_rate)\n",
    "    return dict(zip(quantiles_levels, quantiles))\n",
    "  \n",
    "# approxQuantile() : Calculates the approximate quantiles of a numerical column of a DataFrame\n",
    "# dict(zip()) : {'food': 'spam', 'age': 42, 'name': 'Monty'}\n",
    "# quantile: value 형태로 반환\n",
    "# udf는 우리가 필요한 새로운 컬럼 기반의 함수를 만들어준다. spark에서 사용자 정의 함수는 functions 객체의 udf 함수로 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REG = 10\n",
    "REG = 0\n",
    "ctr_udf = F.udf(lambda clicks, views: clicks / float(views + REG), FloatType())\n",
    "\n",
    "# ctr_udf : ctr을 구하는 사용자 정의 함수\n",
    "# REG : Regularized Click Probability, 아래 링크 참고\n",
    "# penalizes ads with small amounts of data, making it prefer an ad with large amounts of training data and a reliable probability\n",
    "# http://cs229.stanford.edu/proj2016/report/Guo-PredictCommercialPromotedContentsWillBeClickedByUser-report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by ad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_id_popularity_df = train_set_df.groupby('ad_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "# F.count('*') : 그룹에 포함된 항목 개수를 반환\n",
    "# withColumn(x, y) : x는 컬럼명, y는 컬럼(형태)\n",
    "# ad_id_popularity_df는, ad_id로 묶고 clicks와 views 계산 -> ad_id 별로 ctr 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+----------+\n",
      "| ad_id|clicks|views|       ctr|\n",
      "+------+------+-----+----------+\n",
      "|235447|   923|13842|0.06668112|\n",
      "|134205|    78|  244|0.31967214|\n",
      "|324400|     9|   87|0.10344828|\n",
      "+------+------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+------+-----+----------+\n",
      "| ad_id|clicks|views|       ctr|\n",
      "+------+------+-----+----------+\n",
      "|235447|   923|13842|0.06668112|\n",
      "|134205|    78|  244|0.31967214|\n",
      "|324400|     9|   87|0.10344828|\n",
      "+------+------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad_id_popularity_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418295"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ad_id_popularity_df 요소 개수 반환\n",
    "ad_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(ad_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(ad_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad_id_popularity는 ad_id별 ctr을 가지고 있는 ad_id_popularity_df에서 views 수가 5 이상인 것 추출\n",
    "# ad_id를 key로 (ctr, views, 1, 1) 반환\n",
    "\n",
    "ad_id_popularity = ad_id_popularity_df.filter('views > 5').select('ad_id', 'ctr', 'views') \\\n",
    "                    .rdd.map(lambda x: (x['ad_id'], (x['ctr'], x['views'], 1, 1))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.broadcast : 스파크는 효과적인 브로드캐스트 알고리즘을 이용하여 커뮤니케이션의 비용을 줄이는 방향으로 분배를 시도한다\n",
    "\n",
    "ad_id_popularity_broad = sc.broadcast(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ad_id_popularity.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.07692307978868484, 13, 1, 1),\n",
       " (0.08661417663097382, 127, 1, 1),\n",
       " (0.11999999731779099, 25, 1, 1)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ad_id_popularity의 value들을 리스트로 담고 최초 3개 출력\n",
    "\n",
    "list(ad_id_popularity.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192107"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ad_id_popularity의 길이 반환\n",
    "\n",
    "len(ad_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ad_id_ctr_udf = F.udf(lambda ad_id: ad_id_popularity[ad_id] if ad_id in ad_id_popularity else -1, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1552830593979102"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ad_id 전체의 avg_ctr, ad_id별 ctr을 다 더해서 전체 개수로 나눔\n",
    "# map(lambda()) 형태의 이해 ↓\n",
    "# Input : li = [1, 2, 3]\n",
    "# Output : result = [1, 4, 9]\n",
    "# 풀이 : result = list(map(lambda i: i ** 2 , li))\n",
    "\n",
    "ad_id_avg_ctr = sum(map(lambda x: x[0], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19405337738275"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ad_id의 weighted_avg_ctr\n",
    "\n",
    "ad_id_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], ad_id_popularity.values())) / float(sum(map(lambda x: x[1], ad_id_popularity.values())))\n",
    "ad_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_median = np.median(np.array(list(map(lambda x: x[1], ad_id_popularity.values()))))\n",
    "ad_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.59291436543174"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_id_views_mean = sum(map(lambda x: x[1], ad_id_popularity.values())) / float(len(ad_id_popularity))\n",
    "ad_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by document_id (promoted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74766"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_popularity_df = train_set_df.groupby('document_id_promo').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                               F.count('*').alias('views'),\n",
    "                                                               F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "    \n",
    "document_id_popularity = document_id_popularity_df.filter('views > 5').select('document_id_promo', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                                                .rdd.map(lambda x: (x['document_id_promo'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(document_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+-----+---------------+----------+\n",
      "|document_id_promo|clicks|views|distinct_ad_ids|       ctr|\n",
      "+-----------------+------+-----+---------------+----------+\n",
      "|          1071762|   133|  329|             19|0.40425533|\n",
      "|           959407|    10|  121|             14|0.08264463|\n",
      "|           971908|  1472| 8640|             37|0.17037037|\n",
      "+-----------------+------+-----+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_id_popularity_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_popularity_broad = sc.broadcast(document_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(document_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(document_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15048812200823822"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_avg_ctr = sum(map(lambda x: x[0], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19380676920446974"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_weighted_avg_ctr = sum(list(map(lambda x: x[0]*x[1], document_id_popularity.values()))) / float(sum(list(map(lambda x: x[1], document_id_popularity.values()))))\n",
    "document_id_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_median = np.median(np.array(list(map(lambda x: x[1], document_id_popularity.values()))))\n",
    "document_id_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797.3970253858706"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id_views_mean = sum(map(lambda x: x[1], document_id_popularity.values())) / float(len(document_id_popularity))\n",
    "document_id_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by (doc_event, doc_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302421"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1302421"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_event_doc_ad_avg_ctr_df = train_set_df.groupBy('document_id_event', 'document_id_promo') \\\n",
    "                                    .agg(F.sum('clicked').alias('clicks'), \n",
    "                                         F.count('*').alias('views'),\n",
    "                                         F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                    .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "doc_event_doc_ad_avg_ctr = doc_event_doc_ad_avg_ctr_df.filter('views > 5') \\\n",
    "                    .select('document_id_event', 'document_id_promo','ctr', 'views', 'distinct_ad_ids') \\\n",
    "                    .rdd.map(lambda x: ((x['document_id_event'], x['document_id_promo']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()        \n",
    "\n",
    "len(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------+-----+---------------+----------+\n",
      "|document_id_event|document_id_promo|clicks|views|distinct_ad_ids|       ctr|\n",
      "+-----------------+-----------------+------+-----+---------------+----------+\n",
      "|           829678|          1060089|     6|   25|              2|      0.24|\n",
      "|          1586626|          1705546|    45|  210|              1|0.21428572|\n",
      "|          2054862|          1409794|    16|  676|              1|0.02366864|\n",
      "+-----------------+-----------------+------+-----+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-----------------+------+-----+---------------+----------+\n",
      "|document_id_event|document_id_promo|clicks|views|distinct_ad_ids|       ctr|\n",
      "+-----------------+-----------------+------+-----+---------------+----------+\n",
      "|           829678|          1060089|     6|   25|              2|      0.24|\n",
      "|          1586626|          1705546|    45|  210|              1|0.21428572|\n",
      "|          2054862|          1409794|    16|  676|              1|0.02366864|\n",
      "+-----------------+-----------------+------+-----+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_event_doc_ad_avg_ctr_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_event_doc_ad_avg_ctr_broad = sc.broadcast(doc_event_doc_ad_avg_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by country, source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29856"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_popularity_df = train_set_df.select('clicked', 'source_id', 'event_country', 'ad_id') \\\n",
    "                                            .groupby('event_country', 'source_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()\n",
    "source_id_by_country_popularity = source_id_by_country_popularity_df.filter('views > 5 and source_id is not null and event_country <> \"\"').select('event_country', 'source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "        .rdd.map(lambda x: ((x['event_country'], x['source_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CO', 13256),\n",
       " (u'EG', 9414),\n",
       " (u'MX', 6560),\n",
       " (u'FI', 11808),\n",
       " (u'BD', 144),\n",
       " (u'KR', 2413),\n",
       " (u'PH', 10029),\n",
       " (u'GB', 11739),\n",
       " (u'US', 10127),\n",
       " (u'CN', 3421)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(source_id_by_country_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.28018224239349365, 439, 3, 1),\n",
       " (0.2083333283662796, 24, 1, 1),\n",
       " (0.24137930572032928, 29, 1, 1),\n",
       " (0.08588957041501999, 163, 9, 1),\n",
       " (0.056338027119636536, 71, 2, 1),\n",
       " (0.23880596458911896, 67, 12, 1),\n",
       " (0.12181111425161362, 5919, 34, 1),\n",
       " (0.10822510719299316, 231, 3, 1),\n",
       " (0.2380952388048172, 21, 3, 1),\n",
       " (0.0, 12, 1, 1)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(source_id_by_country_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_by_country_popularity_broad = sc.broadcast(source_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18602978924640837"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_avg_ctr = sum(map(lambda x: x[0], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19364919749745055"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_weighted_avg_ctr = sum(map(lambda x: x[0]*x[1], source_id_by_country_popularity.values())) / float(sum(map(lambda x: x[1], source_id_by_country_popularity.values())))\n",
    "source_id_by_country_weighted_avg_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_median = np.median(np.array(list(map(lambda x: x[1], source_id_by_country_popularity.values()))))\n",
    "source_id_by_country_views_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999.1523311897106"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_by_country_views_mean = sum(map(lambda x: x[1], source_id_by_country_popularity.values())) / float(len(source_id_by_country_popularity))\n",
    "source_id_by_country_views_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5628"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_popularity_df = train_set_df.select('clicked', 'source_id', 'ad_id') \\\n",
    "                                            .groupby('source_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                     F.count('*').alias('views'),\n",
    "                                                                     F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "source_id_popularity = source_id_popularity_df.filter('views > 10 and source_id is not null').select('source_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['source_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(source_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_popularity_broad = sc.broadcast(source_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 9, 10, 13, 14]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(source_id_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.185546875, 512, 12, 1),\n",
       " (0.2708333432674408, 48, 7, 1),\n",
       " (0.1348569095134735, 9714, 94, 1),\n",
       " (0.10801393538713455, 1435, 23, 1),\n",
       " (0.2247454673051834, 54119, 53, 1),\n",
       " (0.20213228464126587, 13788, 39, 1),\n",
       " (0.115292027592659, 19108, 129, 1),\n",
       " (0.1080050840973854, 787, 24, 1),\n",
       " (0.18909889459609985, 11375, 6, 1),\n",
       " (0.07083333283662796, 480, 3, 1)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(source_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_id_popularity_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(source_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(source_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_id_popularity = source_id_popularity_df.filter('views > 100 and source_id is not null').select('source_id', 'ctr').rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by publisher_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_popularity_df = train_set_df.select('clicked', 'publisher_id', 'ad_id') \\\n",
    "                                            .groupby('publisher_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                              F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "publisher_popularity = publisher_popularity_df.filter('views > 10 and publisher_id is not null').select('publisher_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['publisher_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(publisher_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_popularity_broad = sc.broadcast(publisher_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 9, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(publisher_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.20435592532157898, 20386, 58, 1),\n",
       " (0.2170737385749817, 12733, 9, 1),\n",
       " (0.20462098717689514, 6319, 10, 1),\n",
       " (0.46600332856178284, 11457, 59, 1),\n",
       " (0.13055281341075897, 8104, 32, 1),\n",
       " (0.2990143597126007, 172373, 5895, 1),\n",
       " (0.0833333358168602, 12, 1, 1),\n",
       " (0.1080050840973854, 787, 24, 1),\n",
       " (0.28918322920799255, 453, 3, 1),\n",
       " (0.24315619468688965, 1242, 1, 1)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(publisher_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publisher_popularity_df.count()\n",
    "##863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(publisher_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(publisher_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publisher_id_popularity = publisher_popularity_df.filter('views > 100 and publisher_id is not null').select('publisher_id', 'ctr').rdd.collectAsMap()\n",
    "#len(publisher_id_popularity)\n",
    "##639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by advertiser_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advertiser_id_popularity_df = train_set_df.select('clicked', 'advertiser_id', 'ad_id') \\\n",
    "                                            .groupby('advertiser_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                          F.count('*').alias('views'),\n",
    "                                                                          F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "advertiser_id_popularity = advertiser_id_popularity_df.filter('views > 10 and advertiser_id is not null').select('advertiser_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['advertiser_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(advertiser_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertiser_id_popularity_broad = sc.broadcast(advertiser_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 12]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(advertiser_id_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06451612710952759, 31, 2, 1),\n",
       " (0.17577283084392548, 5564, 12, 1),\n",
       " (0.2574625015258789, 21005, 153, 1),\n",
       " (0.19177444279193878, 143820, 41, 1),\n",
       " (0.1421319842338562, 197, 14, 1),\n",
       " (0.20100592076778412, 5567, 87, 1),\n",
       " (0.2995377779006958, 32884, 440, 1),\n",
       " (0.3259281814098358, 26423, 224, 1),\n",
       " (0.2201203852891922, 1163, 6, 1),\n",
       " (0.31559163331985474, 63053, 18, 1)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(advertiser_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#advertiser_id_popularity_df.count()\n",
    "##4063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(advertiser_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(advertiser_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#advertiser_id_popularity = advertiser_id_popularity_df.filter('views > 100 and advertiser_id is not null').select('advertiser_id', 'ctr').rdd.collectAsMap()\n",
    "#len(advertiser_id_popularity)\n",
    "##3129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by campaign_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25270"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaign_id_popularity_df = train_set_df.select('clicked', 'campaign_id', 'ad_id') \\\n",
    "                                            .groupby('campaign_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                        F.count('*').alias('views'),\n",
    "                                                                        F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "        \n",
    "campaign_id_popularity = campaign_id_popularity_df.filter('views > 10 and campaign_id is not null').select('campaign_id', 'ctr', 'views', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['campaign_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], 1))).collectAsMap()\n",
    "len(campaign_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_id_popularity_broad = sc.broadcast(campaign_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 10, 11]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(campaign_id_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.23687681555747986, 3791, 28, 1),\n",
       " (0.11906449496746063, 1411, 54, 1),\n",
       " (0.12451361864805222, 257, 2, 1),\n",
       " (0.2747933864593506, 6292, 201, 1),\n",
       " (0.02298850566148758, 87, 12, 1),\n",
       " (0.3658314347267151, 2195, 4, 1),\n",
       " (0.03232758492231369, 464, 8, 1),\n",
       " (0.09447415173053741, 561, 23, 1),\n",
       " (0.05645161122083664, 124, 19, 1),\n",
       " (0.2094017118215561, 234, 15, 1)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(campaign_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#campaign_id_popularity_df.count()\n",
    "##31390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(campaign_id_popularity_df, 'clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_percentiles(campaign_id_popularity_df, 'views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#campaign_id_popularity = campaign_id_popularity_df.filter('views > 100 and campaign_id is not null').select('campaign_id', 'ctr').rdd.collectAsMap()\n",
    "#len(campaign_id_popularity)\n",
    "##16097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_id_popularity_df = train_set_df.join(documents_categories_df.alias('cat_local'), on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "                                        .select('clicked', 'category_id', 'confidence_level_cat', 'ad_id') \\\n",
    "                                        .groupby('category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                    F.count('*').alias('views'),\n",
    "                                                                    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "                                                                    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "category_id_popularity = category_id_popularity_df.filter('views > 10').select('category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['category_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()\n",
    "len(category_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_popularity_broad = sc.broadcast(category_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2100, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.20317520201206207, 2629381, 25586, 0.4870458696523287),\n",
       " (0.2013949453830719, 10323, 44, 0.5),\n",
       " (0.2029319703578949, 1008811, 9187, 0.3881761040784007),\n",
       " (0.1313016414642334, 727310, 5295, 0.2439268331086716),\n",
       " (0.17418651282787323, 920519, 6850, 0.5333781843415426),\n",
       " (0.25920549035072327, 78513, 610, 0.5965200782357787),\n",
       " (0.2357751578092575, 692503, 4638, 0.5506155645951468),\n",
       " (0.24124911427497864, 178782, 2234, 0.5543352849187084),\n",
       " (0.21674630045890808, 1220676, 6280, 0.4995168007977859),\n",
       " (0.19554254412651062, 2172586, 13408, 0.4344011978646628)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692503.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 views의 중앙값\n",
    "np.median(np.array(list(map(lambda x: x[1], category_id_popularity.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246354.6736842105"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 views의 평균, 모든 views를 다 더해서 개수로 나눔\n",
    "sum(map(lambda x: x[1], category_id_popularity.values())) / float(len(category_id_popularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7997823456132471"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1246354.6736842105/692503.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parece haver uma hierarquia nas categorias pelo padrão dos códigos...\n",
    "#기본적으로 카테고리에 계층 구조가 있는 것 같습니다\n",
    "#category_id_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by (country, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10987"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_id_by_country_popularity_df = train_set_df.join(documents_categories_df.alias('cat_local'), on=F.col(\"document_id_promo\") == F.col(\"cat_local.document_id_cat\"), how='inner') \\\n",
    "                                        .select('clicked', 'category_id', 'confidence_level_cat', 'event_country', 'ad_id') \\\n",
    "                                        .groupby('event_country','category_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                                    F.count('*').alias('views'),\n",
    "                                                                                    F.mean('confidence_level_cat').alias('avg_confidence_level_cat'),\n",
    "                                                                                    F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "\n",
    "category_id_by_country_popularity = category_id_by_country_popularity_df.filter('views > 10 and event_country <> \"\"').select('event_country', 'category_id', 'ctr', 'views', 'avg_confidence_level_cat', 'distinct_ad_ids') \\\n",
    "                                     .rdd.map(lambda x: ((x['event_country'], x['category_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_cat']))).collectAsMap()\n",
    "len(category_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CN', 1406),\n",
       " (u'SZ', 1205),\n",
       " (u'VI', 1806),\n",
       " (u'LB', 1512),\n",
       " (u'NZ', 1208),\n",
       " (u'GY', 1209),\n",
       " (u'JO', 1902),\n",
       " (u'BD', 1612),\n",
       " (u'VE', 1710),\n",
       " (u'AP', 1912)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_by_country_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2261904776096344, 84, 35, 0.3214457171658675),\n",
       " (0.0972222238779068, 144, 65, 0.6175835872482922),\n",
       " (0.44999998807907104, 20, 12, 0.3357881270349026),\n",
       " (0.30000001192092896, 30, 6, 0.1443000006179015),\n",
       " (0.25438597798347473, 684, 9, 0.08283828371013814),\n",
       " (0.12121212482452393, 66, 26, 0.7296096152541313),\n",
       " (0.19780220091342926, 91, 3, 0.5724362015396685),\n",
       " (0.31991294026374817, 919, 38, 0.1952407232630629),\n",
       " (0.5, 12, 1, 0.07000000029802322),\n",
       " (0.1538461595773697, 13, 8, 0.3252715221964396)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(category_id_by_country_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_by_country_popularity_broad = sc.broadcast(category_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_id_popularity_df = train_set_df.join(documents_topics_df.alias('top_local'), on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "                                        .select('clicked', 'topic_id', 'confidence_level_top', 'ad_id') \\\n",
    "                                        .groupby('topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                 F.count('*').alias('views'),\n",
    "                                                                 F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "                                                                 F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "topic_id_popularity = topic_id_popularity_df.filter('views > 10').select('topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: (x['topic_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n",
    "len(topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_popularity_broad = sc.broadcast(topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(topic_id_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.22373904287815094, 453287, 2647, 0.12233456528386759),\n",
       " (0.18213681876659393, 504714, 3267, 0.07418431459465535),\n",
       " (0.16457995772361755, 556939, 5764, 0.09277217948046136),\n",
       " (0.2484738975763321, 109593, 1110, 0.1088232609985956),\n",
       " (0.25570839643478394, 98933, 951, 0.05725601736188657),\n",
       " (0.12132325023412704, 289046, 2309, 0.013733238268172656),\n",
       " (0.2379174381494522, 181046, 1129, 0.16137225879193712),\n",
       " (0.15632249414920807, 62934, 523, 0.12299769651588695),\n",
       " (0.2085820436477661, 1491159, 19499, 0.05025825825301043),\n",
       " (0.1834765374660492, 241148, 2707, 0.05404990259875808)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(topic_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526640.4966666667"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 토픽 평균 views : 전체 토픽 views의 합 / 전체 토픽 개수\n",
    "sum(map(lambda x: x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6998657300.896667"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# views * distinct_ad_ids의 합 / 전체 토픽 개수\n",
    "sum(map(lambda x: x[2]*x[1], topic_id_popularity.values())) / float(len(topic_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by (country, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33071"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_id_by_country_popularity_df = train_set_df.join(documents_topics_df.alias('top_local'), on=F.col(\"document_id_promo\") == F.col(\"top_local.document_id_top\"), how='inner') \\\n",
    "                                        .select('clicked', 'topic_id', 'confidence_level_top','event_country', 'ad_id') \\\n",
    "                                        .groupby('event_country','topic_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.mean('confidence_level_top').alias('avg_confidence_level_top'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "topic_id_id_by_country_popularity = topic_id_by_country_popularity_df.filter('views > 10 and event_country <> \"\"').select('event_country', 'topic_id', 'ctr', 'views', 'avg_confidence_level_top', 'distinct_ad_ids') \\\n",
    "                            .rdd.map(lambda x: ((x['event_country'], x['topic_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_top']))).collectAsMap()\n",
    "len(topic_id_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'GA', 249),\n",
       " (u'GH', 101),\n",
       " (u'DE', 291),\n",
       " (u'CL', 133),\n",
       " (u'PA', 140),\n",
       " (u'UA', 3),\n",
       " (u'LR', 82),\n",
       " (u'BM', 198),\n",
       " (u'ZM', 150),\n",
       " (u'BY', 140)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(topic_id_id_by_country_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0833333358168602, 12, 9, 0.0907062344873945),\n",
       " (0.08361203968524933, 299, 10, 0.21701358435644355),\n",
       " (0.1539115607738495, 1176, 62, 0.053913121271663075),\n",
       " (0.4545454680919647, 11, 5, 0.01054142720320008),\n",
       " (0.19164618849754333, 407, 89, 0.18767972210655562),\n",
       " (0.38461539149284363, 13, 4, 0.06967360698259793),\n",
       " (0.2666666805744171, 15, 6, 0.014811059397955736),\n",
       " (0.14601770043373108, 226, 52, 0.13805182779669367),\n",
       " (0.4516128897666931, 31, 11, 0.013377583525594204),\n",
       " (0.2586206793785095, 58, 35, 0.17077118922667256)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(topic_id_id_by_country_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id_id_by_country_popularity_broad = sc.broadcast(topic_id_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78120"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_popularity_df = train_set_df.join(documents_entities_df.alias('ent_local'), on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "                                        .select('clicked', 'entity_id', 'confidence_level_ent', 'ad_id') \\\n",
    "                                        .groupby('entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                  F.count('*').alias('views'),\n",
    "                                                                  F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "                                                                  F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "\n",
    "entity_id_popularity = entity_id_popularity_df.filter('views > 5').select('entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "                                     .rdd.map(lambda x: (x['entity_id'], (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()\n",
    "len(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_popularity_broad = sc.broadcast(entity_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'3e8ea3b1759314b629ccd4cbd059a06f',\n",
       " u'b4dc3ba36d4252202188f7c03cc69f64',\n",
       " u'a6fbaeedbce3404c1c54e9f55608fe7e',\n",
       " u'945cfbbb25dd0d3d9ffa685272bd10cd',\n",
       " u'd7da908ad6f7afb1f783429230b0cf7e',\n",
       " u'b1139aedfdb8e2226e5a6c89253883f4',\n",
       " u'7c006c07568c0c2f6e47a3df621f137a',\n",
       " u'8c8deecf1708520f50cee5311b3c4ff5',\n",
       " u'1fe460f6b59600823c9c9b2938d073bb',\n",
       " u'169ce6cd2c77c3b017c1495f3fb4b00a']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_id_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.13333334028720856, 15, 1, 0.7421413660049438),\n",
       " (0.16304348409175873, 92, 6, 0.24286991357803345),\n",
       " (0.0, 9, 1, 0.9145960211753845),\n",
       " (0.2222222238779068, 45, 4, 0.6937966810332404),\n",
       " (0.23999999463558197, 25, 6, 0.25364906430244444),\n",
       " (0.13786764442920685, 544, 2, 0.38677939772605896),\n",
       " (0.0476190485060215, 21, 4, 0.5017787218093872),\n",
       " (0.125, 8, 1, 0.3097444176673889),\n",
       " (0.125, 8, 3, 0.5733535885810852),\n",
       " (0.17910447716712952, 67, 5, 0.2566896065402387)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_id_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entity별 전체 views의 중앙값\n",
    "np.median(np.array(list(map(lambda x: x[1], entity_id_popularity.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915.9761776753712"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entity 별 views의 합 / entity 개수, 즉 평균\n",
    "sum(map(lambda x: x[1], entity_id_popularity.values())) / float(len(entity_id_popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR by (country, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217703"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_by_country_popularity_df = train_set_df.join(documents_entities_df.alias('ent_local'), on=F.col(\"document_id_promo\") == F.col(\"ent_local.document_id_ent\"), how='inner') \\\n",
    "                                        .select('clicked', 'entity_id', 'event_country', 'confidence_level_ent','ad_id') \\\n",
    "                                        .groupby('event_country','entity_id').agg(F.sum('clicked').alias('clicks'), \n",
    "                                                                             F.count('*').alias('views'),\n",
    "                                                                             F.mean('confidence_level_ent').alias('avg_confidence_level_ent'),\n",
    "                                                                             F.countDistinct('ad_id').alias('distinct_ad_ids')) \\\n",
    "                                         .withColumn('ctr', ctr_udf('clicks','views'))\n",
    "            \n",
    "entity_id_by_country_popularity = entity_id_by_country_popularity_df.filter('views > 5 and event_country <> \"\"').select('event_country', 'entity_id', 'ctr', 'views', 'avg_confidence_level_ent', 'distinct_ad_ids') \\\n",
    "                .rdd.map(lambda x: ((x['event_country'], x['entity_id']), (x['ctr'], x['views'], x['distinct_ad_ids'], x['avg_confidence_level_ent']))).collectAsMap()\n",
    "len(entity_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'SN', u'b55d56364a5c5b0b268321ac5f30552c'),\n",
       " (u'LT', u'c0eed48c55b69811f18ac582895cec93'),\n",
       " (u'AU', u'2b2470e629e3633deaf5179e8f499e80'),\n",
       " (u'GB', u'25f15293eacd8a96042c80e1171210c4'),\n",
       " (u'SG', u'966dbe1dca1afe49d5a7a1795213415b'),\n",
       " (u'US', u'b61639c0c2a77e87802861097a9a79d2'),\n",
       " (u'GB', u'65a87782839eb973ac0cf3dc159c0cd0'),\n",
       " (u'LK', u'786f9062879038e9984db1858fc5c8dd'),\n",
       " (u'SN', u'fa69c242afc1cf0fe4edb74bae206283'),\n",
       " (u'US', u'e4b591efca5fdff326410e9a2a0b0794')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_id_by_country_popularity.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 6, 1, 0.3427562117576599),\n",
       " (0.1666666716337204, 6, 3, 0.47326989471912384),\n",
       " (0.2705882489681244, 85, 8, 0.33241735100746156),\n",
       " (0.1304347813129425, 46, 1, 0.31109827756881714),\n",
       " (0.380952388048172, 21, 3, 0.825154877844311),\n",
       " (0.09090909361839294, 11, 1, 0.833453893661499),\n",
       " (0.12666666507720947, 150, 1, 0.2343740165233612),\n",
       " (0.12935324013233185, 201, 11, 0.019996220245957375),\n",
       " (0.1111111119389534, 9, 1, 0.8727741241455078),\n",
       " (0.0, 8, 2, 0.6611085534095764)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_id_by_country_popularity.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_by_country_popularity_broad = sc.broadcast(entity_id_by_country_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading # docs by categories, topics, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "# import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('aux_data/categories_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "with open('categories_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    categories_docs_counts = cPickle.load(input_file)    \n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('aux_data/topics_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "with open('topics_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    topics_docs_counts = cPickle.load(input_file)\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('aux_data/entities_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "with open('entities_docs_counts'+df_filenames_suffix+'.pickle', 'rb') as input_file:\n",
    "    entities_docs_counts = cPickle.load(input_file)\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'72f364c8af13913d19bd803a3584228c',\n",
       " u'87839ac20cc1d1d9f212ba6bcabab3d2',\n",
       " u'c5b15375624f83017b24f53a09883a3b',\n",
       " u'cd9a81360ff3e85545806c1f4b61932a',\n",
       " u'0e61af952934cb5fdcf2772950bc90d5',\n",
       " u'61e6c101e169f084ff8badb2846856ee',\n",
       " u'1e35fc7b6ee074f40fd49367744b00d6',\n",
       " u'e2b787342f945807c058e86616c1a176',\n",
       " u'0ff307d67fda165a70301531dbf1bfa3',\n",
       " u'e758b2e0d801c21374fa51fff05f71f2']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entities_docs_counts.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 32, 1, 2, 1, 1, 3, 1, 1]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 entity에 해당하는 document_id의 개수\n",
    "list(entities_docs_counts.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Publish Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: 1464109200.0}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_times_df = train_set_df.filter('publish_time is not null').select('document_id_promo','publish_time').distinct().select(F.col('publish_time').cast(IntegerType()))\n",
    "publish_time_percentiles = get_percentiles(publish_times_df, 'publish_time', quantiles_levels=[0.5], max_error_rate=0.001)\n",
    "publish_time_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 5, 24, 17, 0)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publish_time_median = int(publish_time_percentiles[0.5])\n",
    "datetime.datetime.utcfromtimestamp(publish_time_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_diff(newer_timestamp, older_timestamp):\n",
    "    sec_diff = newer_timestamp - older_timestamp\n",
    "    days_diff = sec_diff / 60 / 60 / 24\n",
    "    return days_diff\n",
    "\n",
    "def get_time_decay_factor(timestamp, timestamp_ref=None, alpha=0.001):\n",
    "    if timestamp_ref == None:\n",
    "        timestamp_ref = time.time()\n",
    "        \n",
    "    days_diff = get_days_diff(timestamp_ref, timestamp)\n",
    "    denominator = math.pow(1+alpha, days_diff)\n",
    "    if denominator != 0:\n",
    "        return 1.0 / denominator\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA=1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative)+TIMESTAMP_DELTA)//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DECAY_ALPHA = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(2016, 10, 17, 14, 34, 40), 0.6819865993811997)\n",
      "(datetime.datetime(2016, 9, 24, 14, 34, 40), 0.6741906151274285)\n",
      "(datetime.datetime(2016, 7, 24, 14, 34, 40), 0.653616396779389)\n",
      "(datetime.datetime(2016, 4, 24, 14, 34, 40), 0.6245503815378628)\n",
      "(datetime.datetime(2015, 10, 24, 14, 34, 40), 0.5699535513734337)\n",
      "(datetime.datetime(2014, 10, 24, 14, 34, 40), 0.474898206273678)\n"
     ]
    }
   ],
   "source": [
    "ref_dates = [\n",
    "                1476714880, # 7 days\n",
    "                1474727680, # 30 days\n",
    "                1469370880, # 90 days\n",
    "                1461508480,  # 180 days\n",
    "                1445697280, # 1 year\n",
    "                1414161280 # 2 years\n",
    "]\n",
    "\n",
    "for d in ref_dates:\n",
    "    print(datetime.datetime.utcfromtimestamp(d), get_time_decay_factor(d, alpha=TIME_DECAY_ALPHA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get local time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TZ_EST = -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_utc_bst_tz(event_country, event_country_state):\n",
    "    local_tz = DEFAULT_TZ_EST\n",
    "    if len(event_country) > 0:\n",
    "        if event_country in countries_utc_dst_broad.value:\n",
    "            local_tz = countries_utc_dst_broad.value[event_country]\n",
    "            if len(event_country_state)>2:\n",
    "                state = event_country_state[3:5]\n",
    "                if event_country == 'US':  \n",
    "                    if state in us_states_utc_dst_broad.value:\n",
    "                        local_tz = us_states_utc_dst_broad.value[state]                \n",
    "                elif event_country == 'CA':\n",
    "                    if state in ca_countries_utc_dst_broad.value:\n",
    "                        local_tz = ca_countries_utc_dst_broad.value[state] \n",
    "    return float(local_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_bins_dict = {'EARLY_MORNING': 1,\n",
    "             'MORNING': 2,\n",
    "             'MIDDAY': 3,\n",
    "             'AFTERNOON': 4,\n",
    "             'EVENING': 5,\n",
    "             'NIGHT': 6}\n",
    "\n",
    "hour_bins_values = sorted(hour_bins_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour_bin(hour):\n",
    "    if hour >= 5 and hour < 8:\n",
    "        hour_bin = hour_bins_dict['EARLY_MORNING']\n",
    "    elif hour >= 8 and hour < 11:\n",
    "        hour_bin = hour_bins_dict['MORNING']\n",
    "    elif hour >= 11 and hour < 14:\n",
    "        hour_bin = hour_bins_dict['MIDDAY']\n",
    "    elif hour >= 14 and hour < 19:\n",
    "        hour_bin = hour_bins_dict['AFTERNOON']\n",
    "    elif hour >= 19 and hour < 22:\n",
    "        hour_bin = hour_bins_dict['EVENING']\n",
    "    else:\n",
    "        hour_bin = hour_bins_dict['NIGHT']\n",
    "    return hour_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_datetime(dt, event_country, event_country_state):\n",
    "    local_tz = get_local_utc_bst_tz(event_country, event_country_state)  \n",
    "    tz_delta = local_tz - DEFAULT_TZ_EST\n",
    "    local_time = dt +  datetime.timedelta(hours=tz_delta)\n",
    "    return local_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 11, 22, 3, 56, 53, 619146)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_datetime(datetime.datetime.now(), 'US', 'US>CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(dt):\n",
    "    return dt.weekday() >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_weekend(datetime.datetime(2016, 6, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CTR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('decay_factor_default', 0.9826565333455068)\n"
     ]
    }
   ],
   "source": [
    "timestamp_ref = date_time_to_unix_epoch(datetime.datetime(2016, 6, 29, 3, 59, 59))\n",
    "decay_factor_default = get_time_decay_factor(publish_time_median, timestamp_ref, alpha=TIME_DECAY_ALPHA)\n",
    "print(\"decay_factor_default\", decay_factor_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0)\n",
      "(0.5, 0.024411410743763327)\n",
      "(1, 0.041731582304281624)\n",
      "(2, 0.06614299304804495)\n",
      "(3, 0.08346316460856325)\n",
      "(4, 0.09689773339641579)\n",
      "(5, 0.10787457535232657)\n",
      "(10, 0.14436755531919657)\n",
      "(20, 0.183298356035222)\n",
      "(30, 0.20674645107847822)\n",
      "(100, 0.2778577004917695)\n",
      "(200, 0.3192904933647466)\n",
      "(300, 0.34360197720285013)\n",
      "(1000, 0.41594812296601125)\n",
      "(2000, 0.4576496248565576)\n",
      "(3000, 0.48205100545505175)\n",
      "(10000, 0.5545232830964639)\n",
      "(20000, 0.5962518553291584)\n",
      "(30000, 0.6206622626822822)\n",
      "(50000, 0.6514162003061013)\n",
      "(90000, 0.6868039178501281)\n",
      "(100000, 1.0)\n",
      "(500000, 1.0)\n",
      "(900000, 1.0)\n",
      "(1000000, 1.0)\n",
      "(2171607, 1.0)\n"
     ]
    }
   ],
   "source": [
    "def get_confidence_sample_size(sample, max_for_reference=100000):\n",
    "    #Avoiding overflow for large sample size\n",
    "    if sample >= max_for_reference:\n",
    "        return 1.0\n",
    "\n",
    "    ref_log = math.log(1+max_for_reference, 2) # Curiosly reference in log with base 2(밑이 2인 로그) gives a slightly higher score, so I will keep\n",
    "    \n",
    "    return math.log(1+sample) / float(ref_log)\n",
    "    \n",
    "for i in [0,0.5,1,2,3,4,5,10,20,30,100,200,300,1000,2000,3000,10000,20000,30000, 50000, 90000, 100000, 500000, 900000, 1000000, 2171607]:\n",
    "    print(i, get_confidence_sample_size(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity(an_id, a_dict):\n",
    "    return (a_dict[an_id][0], get_confidence_sample_size(a_dict[an_id][1] / float(a_dict[an_id][2])) * a_dict[an_id][3]) if an_id in a_dict else (None, None)\n",
    "\n",
    "# return (ctr, get_confidence_sample_size(views / distinct_ad_ids) * avg_confidence_level)\n",
    "# get_confidence_sample_size(views / distinct_ad_ids) * avg_confidence_level 이게 뭘까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30364418447489405"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_confidence_sample_size(504714/3267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18213681876659393, 0.022539690355972142)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_popularity(1, topic_id_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_avg_popularity_from_list(ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = list(filter(lambda x: x[0][0]!=None, [(get_popularity(an_id, pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)]))\n",
    "    #print(\"pops\",pops)\n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_avg_country_popularity_from_list(event_country, ids_list, confidence_ids_list, pop_dict):\n",
    "    pops = list(filter(lambda x: x[0][0]!=None, [(get_popularity((event_country, an_id), pop_dict), confidence) for an_id, confidence in zip(ids_list, confidence_ids_list)]))\n",
    "    \n",
    "    if len(pops) > 0:\n",
    "        weighted_avg = sum(map(lambda x: x[0][0]*x[0][1]*x[1], pops)) / float(sum(map(lambda x: x[0][1]*x[1], pops)))\n",
    "        confidence = max(map(lambda x: x[0][1]*x[1], pops))\n",
    "        return weighted_avg, confidence\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                         publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                            output_detailed_list=False):\n",
    "    probs = []\n",
    "    \n",
    "    avg_ctr, confidence = get_popularity(ad_id, ad_id_popularity_broad.value)    \n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_ad_id', avg_ctr, confidence))\n",
    "        \n",
    "    avg_ctr, confidence = get_popularity(document_id, document_id_popularity_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_document_id', avg_ctr, confidence))  \n",
    "        \n",
    "    avg_ctr, confidence = get_popularity((document_id_event, document_id), doc_event_doc_ad_avg_ctr_broad.value)\n",
    "    if avg_ctr != None:\n",
    "        probs.append(('pop_doc_event_doc_ad', avg_ctr, confidence))\n",
    "        \n",
    "        \n",
    "    if source_id != -1:\n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_popularity((event_country, source_id), source_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_popularity(source_id, source_id_popularity_broad.value)        \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_source_id', avg_ctr, confidence))\n",
    "            \n",
    "            \n",
    "    if publisher_id != None:\n",
    "        avg_ctr, confidence = get_popularity(publisher_id, publisher_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_publisher_id', avg_ctr, confidence)) \n",
    "            \n",
    "    if advertiser_id != None:\n",
    "        avg_ctr, confidence = get_popularity(advertiser_id, advertiser_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_advertiser_id', avg_ctr, confidence)) \n",
    "    \n",
    "    if campaign_id != None:\n",
    "        avg_ctr, confidence = get_popularity(campaign_id, campaign_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_campain_id', avg_ctr, confidence))  \n",
    "\n",
    "    if len(entity_ids_by_doc) > 0: \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                                        entity_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                                                                    entity_id_popularity_broad.value) \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_entity_id', avg_ctr, confidence))\n",
    "            \n",
    "    \n",
    "    \n",
    "    if len(topic_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "                                        topic_id_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id_country', avg_ctr, confidence))\n",
    "            \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(topic_ids_by_doc, top_confidence_level_by_doc, \n",
    "                                                                    topic_id_popularity_broad.value)            \n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_topic_id', avg_ctr, confidence))\n",
    "    \n",
    "    \n",
    "    if len(category_ids_by_doc) > 0:  \n",
    "        avg_ctr = None\n",
    "        if event_country != '':\n",
    "            avg_ctr, confidence = get_weighted_avg_country_popularity_from_list(event_country, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        category_id_by_country_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id_country', avg_ctr, confidence))\n",
    "        \n",
    "        avg_ctr, confidence = get_weighted_avg_popularity_from_list(category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                                    category_id_popularity_broad.value)\n",
    "        if avg_ctr != None:\n",
    "            probs.append(('pop_category_id', avg_ctr, confidence))\n",
    "    \n",
    "    #print(\"[get_popularity_score] probs\", probs)\n",
    "    if output_detailed_list:\n",
    "        return probs\n",
    "    \n",
    "    else:    \n",
    "        if len(probs) > 0:\n",
    "            #weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] *  math.log(1+x[2],2), probs)) / float(sum(map(lambda x: math.log(1+x[2],2), probs)))        \n",
    "            weighted_avg_probs_by_confidence = sum(map(lambda x: x[1] * x[2], probs)) / float(sum(map(lambda x: x[2], probs)))                \n",
    "            confidence = max(map(lambda x: x[2], probs))\n",
    "            return weighted_avg_probs_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_dicts(dict1, dict2):\n",
    "    dict1_norm = math.sqrt(sum([v**2 for v in dict1.values()]))\n",
    "    dict2_norm = math.sqrt(sum([v**2 for v in dict2.values()]))\n",
    "    \n",
    "    sum_common_aspects = 0.0\n",
    "    intersections = 0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            sum_common_aspects += dict1[key] * dict2[key] \n",
    "            intersections += 1\n",
    "        \n",
    "    return sum_common_aspects / (dict1_norm * dict2_norm), intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_user_docs_aspects(user_aspect_profile, doc_aspect_ids, doc_aspects_confidence, aspect_docs_counts):\n",
    "    if user_aspect_profile==None or len(user_aspect_profile) == 0 or doc_aspect_ids == None or len(doc_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_aspects = dict(zip(doc_aspect_ids, doc_aspects_confidence))\n",
    "    doc_aspects_tfidf_confid = {}\n",
    "    for key in doc_aspects:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_aspects[key]\n",
    "        doc_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    user_aspects_tfidf_confid = {}    \n",
    "    for key in user_aspect_profile:\n",
    "        tfidf = user_aspect_profile[key][0]\n",
    "        confidence = user_aspect_profile[key][1]\n",
    "        user_aspects_tfidf_confid[key] = tfidf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_aspects_tfidf_confid, user_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_aspects)         / float(len(aspect_docs_counts)), intersections) * \\\n",
    "                       math.pow(len(user_aspect_profile) / float(len(aspect_docs_counts)), intersections)\n",
    "        confidence = 1.0 - random_error\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_aspects) / float(len(aspect_docs_counts))) * \\\n",
    "                            (len(user_aspect_profile) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_doc_event_doc_ad_aspects(doc_event_aspect_ids, doc_event_aspects_confidence, \n",
    "                                               doc_ad_aspect_ids, doc_ad_aspects_confidence, \n",
    "                                               aspect_docs_counts):\n",
    "    if doc_event_aspect_ids == None or len(doc_event_aspect_ids) == 0 or \\\n",
    "       doc_ad_aspect_ids == None or len(doc_ad_aspect_ids) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    doc_event_aspects = dict(zip(doc_event_aspect_ids, doc_event_aspects_confidence))\n",
    "    doc_event_aspects_tfidf_confid = {}\n",
    "    for key in doc_event_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_event_aspects[key]\n",
    "        doc_event_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    doc_ad_aspects = dict(zip(doc_ad_aspect_ids, doc_ad_aspects_confidence))\n",
    "    doc_ad_aspects_tfidf_confid = {}\n",
    "    for key in doc_ad_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_ad_aspects[key]\n",
    "        doc_ad_aspects_tfidf_confid[key] = tf*idf * confidence\n",
    "        \n",
    "    similarity, intersections = cosine_similarity_dicts(doc_event_aspects_tfidf_confid, doc_ad_aspects_tfidf_confid)\n",
    "    \n",
    "    if intersections > 0:\n",
    "        #P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_event_aspect_ids) / float(len(aspect_docs_counts)), intersections) * \\\n",
    "                       math.pow(len(doc_ad_aspect_ids) / float(len(aspect_docs_counts)), intersections)\n",
    "        confidence = 1.0 - random_error\n",
    "    else:\n",
    "        #P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_event_aspect_ids) / float(len(aspect_docs_counts))) * \\\n",
    "                            (len(doc_ad_aspect_ids) / float(len(aspect_docs_counts))))\n",
    "    \n",
    "    confidence = 1.0 - random_error    \n",
    "    \n",
    "    return similarity, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                            timestamp_event, category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                            entity_ids_by_doc, ent_confidence_level_by_doc, \n",
    "                            output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    \n",
    "    sims = []\n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_user_docs_aspects(user_categories, category_ids_by_doc, cat_confidence_level_by_doc, categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_user_docs_aspects(user_topics, topic_ids_by_doc, top_confidence_level_by_doc, topics_docs_counts)\n",
    "    if topics_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "    \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_user_docs_aspects(user_entities, entity_ids_by_doc, ent_confidence_level_by_doc, entities_docs_counts)\n",
    "    if entities_similarity != None:\n",
    "        sims.append(('user_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_event_doc_ad_cb_similarity_score(doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "                                             doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                             doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "                                             doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                             doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                             doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            output_detailed_list=False):\n",
    "\n",
    "    #Content-Based\n",
    "    sims = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    categories_similarity, cat_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_category_ids, doc_event_cat_confidence_levels, \n",
    "                                                    doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                                    categories_docs_counts)\n",
    "    if categories_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_categories', categories_similarity, cat_sim_confidence))\n",
    "    \n",
    "    topics_similarity, top_sim_confidence = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_topic_ids, doc_event_top_confidence_levels, \n",
    "                                                    doc_ad_topic_ids, doc_ad_top_confidence_levels, \n",
    "                                                    topics_docs_counts)\n",
    "    \n",
    "    if topics_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_topics', topics_similarity, top_sim_confidence))\n",
    "        \n",
    "    entities_similarity, entity_sim_confid = cosine_similarity_doc_event_doc_ad_aspects(\n",
    "                                                    doc_event_entity_ids, doc_event_ent_confidence_levels, \n",
    "                                                    doc_ad_entity_ids, doc_ad_ent_confidence_levels, \n",
    "                                                    entities_docs_counts)\n",
    "    \n",
    "    if entities_similarity != None:\n",
    "        sims.append(('doc_event_doc_ad_sim_entities', entities_similarity, entity_sim_confid))\n",
    "    \n",
    "    if output_detailed_list:\n",
    "        return sims\n",
    "    else:\n",
    "        if len(sims) > 0:\n",
    "            weighted_avg_sim_by_confidence = sum(map(lambda x: x[1]*x[2], sims)) / float(sum(map(lambda x: x[2], sims)))\n",
    "            confidence = sum(map(lambda x: x[2], sims)) / float(len(sims))\n",
    "\n",
    "            #print(\"[get_user_cb_interest_score] sims: {} | Avg: {} - Confid: {}\".format(sims, weighted_avg_sim_by_confidence, confidence))\n",
    "            return weighted_avg_sim_by_confidence, confidence\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_feature_names = ['event_weekend',\n",
    "                      'user_has_already_viewed_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_feature_names = ['user_views',\n",
    "                    'ad_views',\n",
    "                    'doc_views',\n",
    "                    'doc_event_days_since_published',\n",
    "                    'doc_event_hour',\n",
    "                    'doc_ad_days_since_published', \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_feature_names = [                                \n",
    "                'pop_ad_id',       \n",
    "                'pop_ad_id_conf',   \n",
    "                'pop_ad_id_conf_multipl', \n",
    "                'pop_document_id',                \n",
    "                'pop_document_id_conf',\n",
    "                'pop_document_id_conf_multipl',\n",
    "                'pop_publisher_id',\n",
    "                'pop_publisher_id_conf',\n",
    "                'pop_publisher_id_conf_multipl',\n",
    "                'pop_advertiser_id',\n",
    "                'pop_advertiser_id_conf',\n",
    "                'pop_advertiser_id_conf_multipl',\n",
    "                'pop_campain_id',\n",
    "                'pop_campain_id_conf',\n",
    "                'pop_campain_id_conf_multipl',\n",
    "                'pop_doc_event_doc_ad',\n",
    "                'pop_doc_event_doc_ad_conf',\n",
    "                'pop_doc_event_doc_ad_conf_multipl',\n",
    "                'pop_source_id',  \n",
    "                'pop_source_id_conf',\n",
    "                'pop_source_id_conf_multipl',\n",
    "                'pop_source_id_country',\n",
    "                'pop_source_id_country_conf',\n",
    "                'pop_source_id_country_conf_multipl',\n",
    "                'pop_entity_id',    \n",
    "                'pop_entity_id_conf',\n",
    "                'pop_entity_id_conf_multipl',\n",
    "                'pop_entity_id_country',\n",
    "                'pop_entity_id_country_conf',\n",
    "                'pop_entity_id_country_conf_multipl',\n",
    "                'pop_topic_id', \n",
    "                'pop_topic_id_conf',\n",
    "                'pop_topic_id_conf_multipl',\n",
    "                'pop_topic_id_country',\n",
    "                'pop_topic_id_country_conf',\n",
    "                'pop_topic_id_country_conf_multipl',\n",
    "                'pop_category_id', \n",
    "                'pop_category_id_conf',\n",
    "                'pop_category_id_conf_multipl',\n",
    "                'pop_category_id_country',\n",
    "                'pop_category_id_country_conf',\n",
    "                'pop_category_id_country_conf_multipl',\n",
    "                'user_doc_ad_sim_categories',    \n",
    "                'user_doc_ad_sim_categories_conf',\n",
    "                'user_doc_ad_sim_categories_conf_multipl',\n",
    "                'user_doc_ad_sim_topics',    \n",
    "                'user_doc_ad_sim_topics_conf',\n",
    "                'user_doc_ad_sim_topics_conf_multipl',\n",
    "                'user_doc_ad_sim_entities',                    \n",
    "                'user_doc_ad_sim_entities_conf',\n",
    "                'user_doc_ad_sim_entities_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_categories',    \n",
    "                'doc_event_doc_ad_sim_categories_conf',\n",
    "                'doc_event_doc_ad_sim_categories_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_topics',    \n",
    "                'doc_event_doc_ad_sim_topics_conf',\n",
    "                'doc_event_doc_ad_sim_topics_conf_multipl',\n",
    "                'doc_event_doc_ad_sim_entities',                    \n",
    "                'doc_event_doc_ad_sim_entities_conf',\n",
    "                'doc_event_doc_ad_sim_entities_conf_multipl'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAFFIC_SOURCE_FV='traffic_source'\n",
    "EVENT_HOUR_FV='event_hour'\n",
    "EVENT_COUNTRY_FV = 'event_country'\n",
    "EVENT_COUNTRY_STATE_FV = 'event_country_state'\n",
    "EVENT_GEO_LOCATION_FV = 'event_geo_location'\n",
    "EVENT_PLATFORM_FV = 'event_platform'\n",
    "AD_ADVERTISER_FV = 'ad_advertiser'\n",
    "DOC_AD_SOURCE_ID_FV='doc_ad_source_id'\n",
    "DOC_AD_PUBLISHER_ID_FV='doc_ad_publisher_id'\n",
    "DOC_EVENT_SOURCE_ID_FV='doc_event_source_id'\n",
    "DOC_EVENT_PUBLISHER_ID_FV='doc_event_publisher_id'\n",
    "DOC_AD_CATEGORY_ID_FV='doc_ad_category_id'\n",
    "DOC_AD_TOPIC_ID_FV='doc_ad_topic_id'\n",
    "DOC_AD_ENTITY_ID_FV='doc_ad_entity_id'\n",
    "DOC_EVENT_CATEGORY_ID_FV='doc_event_category_id'\n",
    "DOC_EVENT_TOPIC_ID_FV='doc_event_topic_id'\n",
    "DOC_EVENT_ENTITY_ID_FV='doc_event_entity_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_feature_names_integral = ['ad_advertiser',\n",
    " 'doc_ad_category_id_1',\n",
    " 'doc_ad_category_id_2',\n",
    " 'doc_ad_category_id_3',\n",
    " 'doc_ad_topic_id_1',\n",
    " 'doc_ad_topic_id_2',\n",
    " 'doc_ad_topic_id_3',\n",
    " 'doc_ad_entity_id_1', \n",
    " 'doc_ad_entity_id_2', \n",
    " 'doc_ad_entity_id_3', \n",
    " 'doc_ad_entity_id_4', \n",
    " 'doc_ad_entity_id_5', \n",
    " 'doc_ad_entity_id_6', \n",
    " 'doc_ad_publisher_id',\n",
    " 'doc_ad_source_id', \n",
    " 'doc_event_category_id_1',\n",
    " 'doc_event_category_id_2',\n",
    " 'doc_event_category_id_3',\n",
    " 'doc_event_topic_id_1',\n",
    " 'doc_event_topic_id_2',\n",
    " 'doc_event_topic_id_3',\n",
    " 'doc_event_entity_id_1',\n",
    " 'doc_event_entity_id_2',\n",
    " 'doc_event_entity_id_3',\n",
    " 'doc_event_entity_id_4',\n",
    " 'doc_event_entity_id_5',\n",
    " 'doc_event_entity_id_6',\n",
    " 'doc_event_publisher_id',\n",
    " 'doc_event_source_id', \n",
    " 'event_country',\n",
    " 'event_country_state',\n",
    " 'event_geo_location',\n",
    " 'event_hour',\n",
    " 'event_platform',\n",
    " 'traffic_source']\n",
    "\n",
    "\n",
    "feature_vector_labels_integral = bool_feature_names + int_feature_names + float_feature_names + \\\n",
    "                                 category_feature_names_integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_labels_integral_dict = dict([(key, idx) for idx, key in enumerate(feature_vector_labels_integral)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feature_vector_labels_integral.txt', 'w') as output:\n",
    "    output.writelines('\\n'.join(feature_vector_labels_integral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value) and str(field_value) != '-1':\n",
    "        feature_name = get_ohe_feature_name(field_name, field_value)\n",
    "        if feature_name in feature_vector_labels_dict:\n",
    "            feature_idx = feature_vector_labels_dict[feature_name]\n",
    "        else:\n",
    "            #Unpopular category value\n",
    "            feature_idx = feature_vector_labels_dict[get_ohe_feature_name(field_name, LESS_SPECIAL_CAT_VALUE)]\n",
    "            \n",
    "        feature_vector[feature_idx] = float(1)\n",
    "        \n",
    "def set_feature_vector_cat_values(field_name, field_values, feature_vector):\n",
    "    for field_value in field_values:\n",
    "        set_feature_vector_cat_value(field_name, field_value, feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                            event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_dict['event_weekend']] = float(weekend)                                                      \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                                publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                                timestamp_event, \n",
    "                                 doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                 doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                 doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "                                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "                                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                        output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        set_feature_vector_cat_value(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_FV, event_country, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_COUNTRY_STATE_FV, event_country_state, feature_vector)         \n",
    "        set_feature_vector_cat_value(EVENT_GEO_LOCATION_FV, geo_location_event, feature_vector)\n",
    "        set_feature_vector_cat_value(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, feature_vector)\n",
    "        set_feature_vector_cat_values(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids, feature_vector)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "                        .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                 event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "                        e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_dict), feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ad_feature_vector_udf = F.udf(lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "                                        user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "                                        doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                        geo_location_event, \n",
    "                                        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                                        traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                        campaign_id, document_id_event,\n",
    "                                        category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                        entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                        doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                        doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                        doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "                                         get_ad_feature_vector(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                                            event_country, event_country_state, \n",
    "                                                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                                            geo_location_event, \n",
    "                                                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                                                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                                            campaign_id, document_id_event,\n",
    "                                                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                                            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                                            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                                            doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                            VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_vector_cat_value_integral(field_name, field_value, feature_vector):\n",
    "    if not is_null(field_value): #and str(field_value) != '-1':\n",
    "        feature_vector[feature_vector_labels_integral_dict[field_name]] = float(field_value)\n",
    "        \n",
    "def set_feature_vector_cat_top_multi_values_integral(field_name, values, confidences, feature_vector, top=5):\n",
    "    top_values = list(filter(lambda z: z != -1, map(lambda y: y[0], sorted(zip(values, confidences), key=lambda x: -x[1]))))[:top]\n",
    "    for idx, field_value in list(enumerate(top_values)):\n",
    "        set_feature_vector_cat_value_integral('{}_{}'.format(field_name, idx+1), field_value, feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                            event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels):\n",
    "             \n",
    "    try:\n",
    "\n",
    "        feature_vector = {}\n",
    "        \n",
    "        if user_views_count != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_views']] = float(user_views_count)\n",
    "         \n",
    "        if user_doc_ids_viewed != None:\n",
    "            feature_vector[feature_vector_labels_integral_dict['user_has_already_viewed_doc']] = float(document_id in user_doc_ids_viewed)               \n",
    "          \n",
    "        if ad_id in ad_id_popularity_broad.value:            \n",
    "            feature_vector[feature_vector_labels_integral_dict['ad_views']] = float(ad_id_popularity_broad.value[ad_id][1])\n",
    "        \n",
    "        if document_id in document_id_popularity_broad.value:\n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_views']] = float(document_id_popularity_broad.value[document_id][1])            \n",
    "            \n",
    "        if timestamp_event > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp_event)\n",
    "            if doc_ad_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_ad_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_ad_days_since_published']] = float(delta_days)\n",
    "                        \n",
    "            if doc_event_publish_time != None:\n",
    "                delta_days = (dt_timestamp_event - doc_event_publish_time).days\n",
    "                if delta_days >= 0 and delta_days <= 365*10: #10 years\n",
    "                    feature_vector[feature_vector_labels_integral_dict['doc_event_days_since_published']] = float(delta_days)\n",
    "                    \n",
    "            \n",
    "            #Local period of the day (hours)\n",
    "            dt_local_timestamp_event = get_local_datetime(dt_timestamp_event, event_country, event_country_state)    \n",
    "            local_hour_bin = get_hour_bin(dt_local_timestamp_event.hour)            \n",
    "            feature_vector[feature_vector_labels_integral_dict['doc_event_hour']] = float(local_hour_bin) #Hour for Decision Trees\n",
    "            set_feature_vector_cat_value_integral(EVENT_HOUR_FV, local_hour_bin, feature_vector) #Period of day for FFM\n",
    "            \n",
    "            #Weekend\n",
    "            weekend = int(is_weekend(dt_local_timestamp_event))\n",
    "            feature_vector[feature_vector_labels_integral_dict['event_weekend']] = float(weekend)               \n",
    "                                        \n",
    "        \n",
    "        conf_field_suffix = '_conf'\n",
    "        conf_multiplied_field_suffix = '_conf_multipl'\n",
    "        \n",
    "        #Setting Popularity fields\n",
    "        pop_scores = get_popularity_score(event_country, ad_id, document_id, source_id, \n",
    "                                publisher_id, advertiser_id, campaign_id, document_id_event,\n",
    "                                doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "        \n",
    "                                \n",
    "\n",
    "        for score in pop_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "\n",
    "        #Setting User-Doc_ad CB Similarity fields\n",
    "        user_doc_ad_cb_sim_scores = get_user_cb_interest_score(user_views_count, user_categories, user_topics, user_entities, \n",
    "                                timestamp_event, \n",
    "                                 doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                 doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                 doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                output_detailed_list=True)\n",
    "\n",
    "        for score in user_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "            \n",
    "        #Setting Doc_event-doc_ad CB Similarity fields\n",
    "        doc_event_doc_ad_cb_sim_scores = get_doc_event_doc_ad_cb_similarity_score(\n",
    "                                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                                            doc_event_entity_ids, doc_event_ent_confidence_levels,\n",
    "                                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                                        output_detailed_list=True)\n",
    "        \n",
    "        for score in doc_event_doc_ad_cb_sim_scores:\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]]] = score[1]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_field_suffix]] = score[2]\n",
    "            feature_vector[feature_vector_labels_integral_dict[score[0]+conf_multiplied_field_suffix]] = score[1] * score[2]\n",
    "        \n",
    "        #Process code for event_country\n",
    "        if event_country in event_country_values_counts:\n",
    "            event_country_code = event_country_values_counts[event_country]\n",
    "        else:\n",
    "            event_country_code = event_country_values_counts[LESS_SPECIAL_CAT_VALUE]                        \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_FV, event_country_code, feature_vector)\n",
    "        \n",
    "        #Process code for event_country_state\n",
    "        if event_country_state in event_country_state_values_counts:\n",
    "            event_country_state_code = event_country_state_values_counts[event_country_state]\n",
    "        else:\n",
    "            event_country_state_code = event_country_state_values_counts[LESS_SPECIAL_CAT_VALUE]         \n",
    "        set_feature_vector_cat_value_integral(EVENT_COUNTRY_STATE_FV, event_country_state_code, feature_vector)\n",
    "                \n",
    "        #Process code for geo_location_event\n",
    "        if geo_location_event in event_geo_location_values_counts:\n",
    "            geo_location_event_code = event_geo_location_values_counts[geo_location_event]\n",
    "        else:\n",
    "            geo_location_event_code = event_geo_location_values_counts[LESS_SPECIAL_CAT_VALUE]\n",
    "        set_feature_vector_cat_value_integral(EVENT_GEO_LOCATION_FV, geo_location_event_code, feature_vector)   \n",
    "         \n",
    "        set_feature_vector_cat_value_integral(TRAFFIC_SOURCE_FV, traffic_source_pv, feature_vector)        \n",
    "        set_feature_vector_cat_value_integral(EVENT_PLATFORM_FV, platform_event, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(AD_ADVERTISER_FV, advertiser_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_SOURCE_ID_FV, source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_AD_PUBLISHER_ID_FV, publisher_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_SOURCE_ID_FV, doc_event_source_id, feature_vector)\n",
    "        set_feature_vector_cat_value_integral(DOC_EVENT_PUBLISHER_ID_FV, doc_event_publisher_id, feature_vector)\n",
    "                \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_CATEGORY_ID_FV, doc_ad_category_ids, doc_ad_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_TOPIC_ID_FV, doc_ad_topic_ids, doc_ad_top_confidence_levels, feature_vector, top=3)\n",
    "        \n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_CATEGORY_ID_FV, doc_event_category_ids, doc_event_cat_confidence_levels, feature_vector, top=3)\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_TOPIC_ID_FV, doc_event_topic_ids, doc_event_top_confidence_levels, feature_vector, top=3)                           \n",
    "        \n",
    "        #Process codes for doc_ad_entity_ids\n",
    "        doc_ad_entity_ids_codes = [doc_entity_id_values_counts[x] if x in doc_entity_id_values_counts \n",
    "                                   else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "                                   for x in doc_ad_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_AD_ENTITY_ID_FV, doc_ad_entity_ids_codes, doc_ad_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        \n",
    "        #Process codes for doc_event_entity_ids\n",
    "        doc_event_entity_ids_codes = [doc_entity_id_values_counts[x] if x in doc_entity_id_values_counts \n",
    "                                   else doc_entity_id_values_counts[LESS_SPECIAL_CAT_VALUE] \n",
    "                                   for x in doc_event_entity_ids]\n",
    "        set_feature_vector_cat_top_multi_values_integral(DOC_EVENT_ENTITY_ID_FV, doc_event_entity_ids_codes, doc_event_ent_confidence_levels, feature_vector, top=6)\n",
    "        \n",
    "        #Creating dummy column as the last column because xgboost have a problem if the last column is undefined for all rows, \n",
    "        #saying that dimentions of data and feature_names do not match\n",
    "        #feature_vector[feature_vector_labels_dict[DUMMY_FEATURE_COLUMN]] = float(0)\n",
    "            \n",
    "        #Ensuring that all elements are floats for compatibility with UDF output (ArrayType(FloatType()))\n",
    "        #feature_vector = list([float(x) for x in feature_vector])\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(\"[get_ad_feature_vector_integral] ERROR PROCESSING FEATURE VECTOR! Params: {}\" \\\n",
    "                        .format([user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                 event_country, event_country_state,\n",
    "                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                            geo_location_event, \n",
    "                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                            campaign_id, document_id_event,\n",
    "                            doc_ad_category_ids, doc_ad_cat_confidence_levels, \n",
    "                            doc_ad_topic_ids, doc_ad_top_confidence_levels,\n",
    "                            doc_ad_entity_ids, doc_ad_ent_confidence_levels,\n",
    "                            doc_event_category_ids, doc_event_cat_confidence_levels,\n",
    "                            doc_event_topic_ids, doc_event_top_confidence_levels,\n",
    "                            doc_event_entity_ids, doc_event_ent_confidence_levels]),\n",
    "                        e)\n",
    "    \n",
    "    return SparseVector(len(feature_vector_labels_integral_dict), feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ad_feature_vector_integral_udf = F.udf(lambda user_doc_ids_viewed, user_views_count, user_categories, user_topics, \n",
    "                                        user_entities, event_country, event_country_state, ad_id, document_id, source_id, \n",
    "                                        doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                        geo_location_event, \n",
    "                                        doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,\n",
    "                                        traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                        campaign_id, document_id_event,\n",
    "                                        category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                        topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                        entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                        doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                        doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                        doc_event_entity_id_list, doc_event_confidence_level_ent: \\\n",
    "                                         get_ad_feature_vector_integral(user_doc_ids_viewed, user_views_count, user_categories, user_topics, user_entities, \n",
    "                                                            event_country, event_country_state, \n",
    "                                                            ad_id, document_id, source_id, doc_ad_publish_time, timestamp_event, platform_event,\n",
    "                                                            geo_location_event, \n",
    "                                                            doc_event_source_id, doc_event_publisher_id, doc_event_publish_time,   \n",
    "                                                            traffic_source_pv, advertiser_id, publisher_id,\n",
    "                                                            campaign_id, document_id_event,\n",
    "                                                            category_ids_by_doc, cat_confidence_level_by_doc, \n",
    "                                                            topic_ids_by_doc, top_confidence_level_by_doc,\n",
    "                                                            entity_ids_by_doc, ent_confidence_level_by_doc,\n",
    "                                                            doc_event_category_id_list, doc_event_confidence_level_cat_list,\n",
    "                                                            doc_event_topic_id_list, doc_event_confidence_level_top,\n",
    "                                                            doc_event_entity_id_list, doc_event_confidence_level_ent),    \n",
    "                            VectorUDT())\n",
    "                             #StructField(\"features\", VectorUDT()))\n",
    "                             #MapType(IntegerType(), FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Train set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_enriched_df = train_set_df \\\n",
    "                             .join(documents_categories_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                             .join(documents_topics_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                             .join(documents_entities_grouped_df, on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                             .join(documents_categories_grouped_df \\\n",
    "                                       .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list') \\\n",
    "                                       .alias('documents_event_categories_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"), \n",
    "                                   how='left') \\\n",
    "                             .join(documents_topics_grouped_df \\\n",
    "                                       .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list') \\\n",
    "                                       .alias('documents_event_topics_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"), \n",
    "                                   how='left') \\\n",
    "                             .join(documents_entities_grouped_df \\\n",
    "                                       .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "                                       .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list') \\\n",
    "                                       .alias('documents_event_entities_grouped'), \n",
    "                                   on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"), \n",
    "                                   how='left') \\\n",
    "                            .select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "                                    'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',\n",
    "                                            'publish_time', 'ad_id','document_id_promo','clicked',   \n",
    "                                           'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "                                            'campaign_id', 'document_id_event',\n",
    "                                            'traffic_source_pv',                                          \n",
    "                                        int_list_null_to_empty_list_udf('doc_event_category_id_list').alias('doc_event_category_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list').alias('doc_event_confidence_level_cat_list'),\n",
    "                                        int_list_null_to_empty_list_udf('doc_event_topic_id_list').alias('doc_event_topic_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list').alias('doc_event_confidence_level_top_list'),\n",
    "                                        str_list_null_to_empty_list_udf('doc_event_entity_id_list').alias('doc_event_entity_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list').alias('doc_event_confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('source_id').alias('source_id'),                                      \n",
    "                                       int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "                                       int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_cat_list').alias('confidence_level_cat_list'), \n",
    "                                       int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_top_list').alias('confidence_level_top_list'), \n",
    "                                       str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_ent_list').alias('confidence_level_ent_list')                                                       \n",
    "                                      ) \\\n",
    "                            .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                            .withColumnRenamed('categories', 'user_categories') \\\n",
    "                            .withColumnRenamed('topics', 'user_topics') \\\n",
    "                            .withColumnRenamed('entities', 'user_entities') \\\n",
    "                            .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "                            .withColumnRenamed('views', 'user_views_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_feature_vectors_df = train_set_enriched_df \\\n",
    "                                .withColumn('feature_vector', \n",
    "                                            #get_ad_feature_vector_udf(\n",
    "                                            get_ad_feature_vector_integral_udf(\n",
    "                                                                'user_doc_ids_viewed',\n",
    "                                                                'user_views_count',\n",
    "                                                                'user_categories', \n",
    "                                                                'user_topics', \n",
    "                                                                'user_entities', \n",
    "                                                                'event_country', \n",
    "                                                                'event_country_state',\n",
    "                                                                'ad_id', \n",
    "                                                                'document_id_promo', \n",
    "                                                                'source_id', \n",
    "                                                                'publish_time', \n",
    "                                                                'timestamp_event', \n",
    "                                                                'platform_event',\n",
    "                                                                'geo_location_event', \n",
    "                                                                'source_id_doc_event', \n",
    "                                                                'publisher_doc_event',\n",
    "                                                                'publish_time_doc_event',\n",
    "                                                                'traffic_source_pv',\n",
    "                                                                'advertiser_id', \n",
    "                                                                'publisher_id',\n",
    "                                                                'campaign_id',\n",
    "                                                                'document_id_event',\n",
    "                                                                'category_id_list', \n",
    "                                                                'confidence_level_cat_list', \n",
    "                                                                'topic_id_list', \n",
    "                                                                'confidence_level_top_list',\n",
    "                                                                'entity_id_list', \n",
    "                                                                'confidence_level_ent_list',\n",
    "                                                                'doc_event_category_id_list',\n",
    "                                                                'doc_event_confidence_level_cat_list',\n",
    "                                                                'doc_event_topic_id_list',\n",
    "                                                                'doc_event_confidence_level_top_list',\n",
    "                                                                'doc_event_entity_id_list',\n",
    "                                                                'doc_event_confidence_level_ent_list')) \\\n",
    "                            .select(F.col('uuid_event').alias('uuid'),\n",
    "                                    'display_id',\n",
    "                                    'ad_id',\n",
    "                                    'document_id_event',\n",
    "                                    F.col('document_id_promo').alias('document_id'),\n",
    "                                    F.col('clicked').alias('label'),\n",
    "                                    'feature_vector') #\\\n",
    "                            #.orderBy('display_id','ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral_eval'\n",
    "else:\n",
    "    train_feature_vector_gcs_folder_name = 'train_feature_vectors_integral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 136 ms, total: 540 ms\n",
      "Wall time: 47min 54s\n"
     ]
    }
   ],
   "source": [
    "%time train_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting integral feature vectors to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_vectors_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+train_feature_vector_gcs_folder_name)\n",
    "# train_feature_vectors_exported_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    train_feature_vector_integral_csv_folder_name = 'train_feature_vectors_integral_eval.csv'\n",
    "else:\n",
    "    train_feature_vector_integral_csv_folder_name = 'train_feature_vectors_integral.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n",
    "    \n",
    "with open(train_feature_vector_integral_csv_folder_name+\".header\", 'w') as output:\n",
    "    output.writelines('\\n'.join(integral_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_vector_to_csv_with_nulls_row(additional_column_values, vec, num_columns):    \n",
    "    return ','.join([str(value) for value in additional_column_values] + \n",
    "                     list([ '{:.5}'.format(vec[x]) if x in vec.indices else '' for x in range(vec.size) ])[:num_columns]) \\\n",
    "            .replace('.0,',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_vectors_integral_csv_rdd = train_feature_vectors_exported_df.select(\n",
    "     'label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'feature_vector').withColumn('is_leak', F.lit(-1)) \\\n",
    "     .rdd.map(lambda x: sparse_vector_to_csv_with_nulls_row([x['label'], x['display_id'], x['ad_id'], x['document_id'], x['document_id_event'], x['is_leak']], \n",
    "                                                  x['feature_vector'], len(integral_headers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 412 ms, sys: 212 ms, total: 624 ms\n",
      "Wall time: 57min 38s\n"
     ]
    }
   ],
   "source": [
    "%time train_feature_vectors_integral_csv_rdd.saveAsTextFile(OUTPUT_BUCKET_FOLDER+train_feature_vector_integral_csv_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Validation/Test set feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leak(max_timestamp_pv_leak, timestamp_event):\n",
    "    return max_timestamp_pv_leak >= 0 and max_timestamp_pv_leak >= timestamp_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_leak_udf = F.udf(lambda max_timestamp_pv_leak, timestamp_event: int(is_leak(max_timestamp_pv_leak, timestamp_event)), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    data_df = validation_set_df\n",
    "else:\n",
    "    data_df = test_set_df\n",
    "\n",
    "\n",
    "test_validation_set_enriched_df = data_df.select('display_id','uuid_event','event_country','event_country_state','platform_event',\n",
    "                                            'source_id_doc_event', 'publisher_doc_event','publish_time_doc_event',     \n",
    "                                            'publish_time',\n",
    "                                           'ad_id','document_id_promo','clicked',  \n",
    "                                           'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "                                           'campaign_id', 'document_id_event',\n",
    "                                           'traffic_source_pv',                                           \n",
    "                                        int_list_null_to_empty_list_udf('doc_event_category_id_list').alias('doc_event_category_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_cat_list').alias('doc_event_confidence_level_cat_list'),\n",
    "                                        int_list_null_to_empty_list_udf('doc_event_topic_id_list').alias('doc_event_topic_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_top_list').alias('doc_event_confidence_level_top_list'),\n",
    "                                        str_list_null_to_empty_list_udf('doc_event_entity_id_list').alias('doc_event_entity_id_list'),\n",
    "                                        float_list_null_to_empty_list_udf('doc_event_confidence_level_ent_list').alias('doc_event_confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('source_id').alias('source_id'),                                   \n",
    "                                       int_null_to_minus_one_udf('timestamp_event').alias('timestamp_event'),\n",
    "                                       int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_cat_list').alias('confidence_level_cat_list'), \n",
    "                                       int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_top_list').alias('confidence_level_top_list'), \n",
    "                                       str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                                       float_list_null_to_empty_list_udf('confidence_level_ent_list').alias('confidence_level_ent_list'),\n",
    "                                       int_null_to_minus_one_udf('max_timestamp_pv').alias('max_timestamp_pv_leak')\n",
    "                                      ) \\\n",
    "                            .join(user_profiles_df, on=[F.col(\"user_profiles.uuid\") == F.col(\"uuid_event\")], how='left') \\\n",
    "                            .withColumnRenamed('categories', 'user_categories') \\\n",
    "                            .withColumnRenamed('topics', 'user_topics') \\\n",
    "                            .withColumnRenamed('entities', 'user_entities') \\\n",
    "                            .withColumnRenamed('doc_ids', 'user_doc_ids_viewed') \\\n",
    "                            .withColumnRenamed('views', 'user_views_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_set_feature_vectors_df = test_validation_set_enriched_df \\\n",
    "                                .withColumn('feature_vector', \n",
    "                                            #get_ad_feature_vector_udf(\n",
    "                                            get_ad_feature_vector_integral_udf(\n",
    "                                                                'user_doc_ids_viewed', \n",
    "                                                                'user_views_count',\n",
    "                                                                'user_categories', \n",
    "                                                                'user_topics', \n",
    "                                                                'user_entities', \n",
    "                                                                'event_country', \n",
    "                                                                'event_country_state',\n",
    "                                                                'ad_id', \n",
    "                                                                'document_id_promo', \n",
    "                                                                'source_id', \n",
    "                                                                'publish_time', \n",
    "                                                                'timestamp_event', \n",
    "                                                                'platform_event',\n",
    "                                                                'geo_location_event', \n",
    "                                                                'source_id_doc_event', \n",
    "                                                                'publisher_doc_event',\n",
    "                                                                'publish_time_doc_event',\n",
    "                                                                'traffic_source_pv',\n",
    "                                                                'advertiser_id', \n",
    "                                                                'publisher_id',\n",
    "                                                                'campaign_id',\n",
    "                                                                'document_id_event',\n",
    "                                                                'category_id_list', \n",
    "                                                                'confidence_level_cat_list', \n",
    "                                                                'topic_id_list', \n",
    "                                                                'confidence_level_top_list',\n",
    "                                                                'entity_id_list', \n",
    "                                                                'confidence_level_ent_list',\n",
    "                                                                'doc_event_category_id_list',\n",
    "                                                                'doc_event_confidence_level_cat_list',\n",
    "                                                                'doc_event_topic_id_list',\n",
    "                                                                'doc_event_confidence_level_top_list',\n",
    "                                                                'doc_event_entity_id_list',\n",
    "                                                                'doc_event_confidence_level_ent_list')) \\\n",
    "                            .select(F.col('uuid').alias('uuid'),                                    \n",
    "                                    'display_id',\n",
    "                                    'ad_id',\n",
    "                                    'document_id_event',\n",
    "                                    F.col('document_id_promo').alias('document_id'),\n",
    "                                    F.col('clicked').alias('label'),\n",
    "                                    is_leak_udf('max_timestamp_pv_leak','timestamp_event').alias('is_leak'),\n",
    "                                    'feature_vector') #\\\n",
    "                            #.orderBy('display_id','ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'validation_feature_vectors_integral'\n",
    "else:\n",
    "    test_validation_feature_vector_gcs_folder_name = 'test_feature_vectors_integral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 220 ms, total: 616 ms\n",
      "Wall time: 46min 18s\n"
     ]
    }
   ],
   "source": [
    "%time test_validation_set_feature_vectors_df.write.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting integral feature vectors to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_feature_vectors_exported_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_gcs_folder_name)\n",
    "# test_validation_feature_vectors_exported_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    test_validation_feature_vector_integral_csv_folder_name = 'validation_feature_vectors_integral.csv'\n",
    "else:\n",
    "    test_validation_feature_vector_integral_csv_folder_name = 'test_feature_vectors_integral.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_headers = ['label', 'display_id', 'ad_id', 'doc_id', 'doc_event_id', 'is_leak'] + feature_vector_labels_integral\n",
    "    \n",
    "with open(test_validation_feature_vector_integral_csv_folder_name+\".header\", 'w') as output:\n",
    "    output.writelines('\\n'.join(integral_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation_feature_vectors_integral_csv_rdd = test_validation_feature_vectors_exported_df.select(\n",
    "     'label', 'display_id', 'ad_id', 'document_id', 'document_id_event', 'is_leak', 'feature_vector') \\\n",
    "     .rdd.map(lambda x: sparse_vector_to_csv_with_nulls_row([x['label'], x['display_id'], x['ad_id'], x['document_id'], x['document_id_event'], x['is_leak']], \n",
    "                                                  x['feature_vector'], len(integral_headers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 228 ms, sys: 92 ms, total: 320 ms\n",
      "Wall time: 29min 11s\n"
     ]
    }
   ],
   "source": [
    "%time test_validation_feature_vectors_integral_csv_rdd.saveAsTextFile(OUTPUT_BUCKET_FOLDER+test_validation_feature_vector_integral_csv_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,3110701,149004,1335842,1973614,0,0,0,43,5042,1.5296e+04,0,3,54,0.25823,0.5133,0.13255,0.22882,0.53838,0.12319,0.22882,0.53838,0.12319,0.18547,0.34324,0.063661,0.23782,0.33201,0.078959,,,,0.22882,0.53838,0.12319,0.22882,0.53838,0.12319,0.22881,0.4539,0.10386,0.22881,0.4539,0.10386,0.19476,0.017983,0.0035023,0.2483,0.016695,0.0041453,0.18375,0.14213,0.026118,0.18861,0.14897,0.028097,0.064491,0.99998,0.06449,0,0.00047778,0,0,5.1186e-11,0,0.013609,0.99957,0.013603,0,1.1111e-05,0,0,1.1374e-12,0,709,1100,1408,,277,,,,,,,,,509,3890,1403,1408,,136,,,,,,,,,407,6715,1.8595e+07,1.6453e+06,4.2814e+05,3,2,1.0']\n"
     ]
    }
   ],
   "source": [
    "print(test_validation_feature_vectors_integral_csv_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,3110701,149004,1335842,1973614,0,0,0,43,5042,1.5296e+04,0,3,54,0.25823,0.5133,0.13255,0.22882,0.53838,0.12319,0.22882,0.53838,0.12319,0.18547,0.34324,0.063661,0.23782,0.33201,0.078959,,,,0.22882,0.53838,0.12319,0.22882,0.53838,0.12319,0.22881,0.4539,0.10386,0.22881,0.4539,0.10386,0.19476,0.017983,0.0035023,0.2483,0.016695,0.0041453,0.18375,0.14213,0.026118,0.18861,0.14897,0.028097,0.064491,0.99998,0.06449,0,0.00047778,0,0,5.1186e-11,0,0.013609,0.99957,0.013603,0,1.1111e-05,0,0,1.1374e-12,0,709,1100,1408,,277,,,,,,,,,509,3890,1403,1408,,136,,,,,,,,,407,6715,1.8595e+07,1.6453e+06,4.2814e+05,3,2,1.0\n"
     ]
    }
   ],
   "source": [
    "print(test_validation_feature_vectors_integral_csv_rdd.first())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
