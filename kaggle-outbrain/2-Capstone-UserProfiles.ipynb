{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 작업은, 코드를 입력하고 실행하면 바로 실행되는 것이 아니다\n",
    "# 지연 연산(lazy evaluation)을 사용해서 기초 데이터에 적용될 변환 연산을 기억하고 있고 동작(Action)이 실행될 때 한 번에 실행된다\n",
    "# 이런 과정을 통해 스파크가 자동으로 최적화 및 장애나 느리게 작업하는 일꾼을 깔끔하게 처리한다\n",
    "\n",
    "evaluation = True\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://capstone-01/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://capstone-01/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 함수 udf\n",
    "# timestamp를 넣으면 일자(숫자) 반환\n",
    "# 예를 들어, events.csv의 timestamp를 이 함수에 넣으면, ad를 클릭한 날이 기준일로부터 몇 일인지 반환 (0~12) \n",
    "\n",
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip 함수로 데이터의 양쪽 공백을 지우고, 앞에서 2자만 떼어내서 반환하는 함수 (데이터가 None이면 '' 반환)\n",
    "\n",
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_meta.csv 가져오기\n",
    "# dummyDocumentsMeta 컬럼 추가\n",
    "# withColumn('컬럼명', F.lit(1)) -> 더미변수 만드는 함수, 하나의 컬럼으로 추가됨\n",
    "# F.lit() : Creates a Column of literal value, lit 함수는 상수 컬럼을 만들 때 사용한다\n",
    "# you have to use lit if you want to access any of the pyspark.sql.Column methods treating standard Python scalar as a constant column\n",
    "\n",
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+-------------------+------------------+\n",
      "|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|\n",
      "+---------------+---------+------------+-------------------+------------------+\n",
      "|        1595802|        1|         603|2016-06-05 00:00:00|                 1|\n",
      "|        1524246|        1|         603|2016-05-26 11:00:00|                 1|\n",
      "|        1617787|        1|         603|2016-05-27 00:00:00|                 1|\n",
      "|        1615583|        1|         603|2016-06-07 00:00:00|                 1|\n",
      "|        1615460|        1|         603|2016-06-20 00:00:00|                 1|\n",
      "|        1615354|        1|         603|2016-06-10 00:00:00|                 1|\n",
      "|        1614611|        1|         603|2016-06-05 13:00:00|                 1|\n",
      "|        1614235|        1|         603|2016-06-09 00:00:00|                 1|\n",
      "|        1614225|        1|         603|2016-06-09 00:00:00|                 1|\n",
      "|        1488264|        1|         603|2016-05-23 13:00:00|                 1|\n",
      "+---------------+---------+------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_meta_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_categories.csv 가져오기 \n",
    "\n",
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories')\n",
    "\n",
    "# documents_categories_grouped_df 생성하기\n",
    "# agg 함수에는 원하는 기능과 컬럼을 전달한다\n",
    "# 데이터들을 document_id로 그룹화하고, 한 document_id에 해당하는 category_id와 confidence_level을 각각 리스트로 담는다  \n",
    "# F.collect_list() : 주어진 컬럼의 모든 값을 수집하여 하나의 리스트로 만듦\n",
    "# dummyDocumentsCategory 컬럼 추가 (category 더미변수 생성)\n",
    "\n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "                                                 F.collect_list('confidence_level_cat').alias('cat_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "                                            .alias('documents_categories_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------------+\n",
      "|document_id_cat|category_id|confidence_level_cat|\n",
      "+---------------+-----------+--------------------+\n",
      "|        1595802|       1611|                0.92|\n",
      "|        1595802|       1610|                0.07|\n",
      "|        1524246|       1807|                0.92|\n",
      "|        1524246|       1608|                0.07|\n",
      "|        1617787|       1807|                0.92|\n",
      "|        1617787|       1608|                0.07|\n",
      "|        1615583|       1305|                0.92|\n",
      "|        1615583|       1806|                0.07|\n",
      "|        1615460|       1613|           0.5406464|\n",
      "|        1615460|       1603|          0.04113614|\n",
      "+---------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_categories_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_topics.csv 가져오기 \n",
    "\n",
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics')\n",
    "    \n",
    "# documents_topics_grouped_df 생성하기  \n",
    "# 데이터들을 document_id로 그룹화하고, 한 document_id에 해당하는 topic_id와 confidence_level을 각각 리스트로 담는다 \n",
    "# dummyDocumentsTopics 컬럼 추가 (topics 더미변수 생성)\n",
    "\n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "                                                 F.collect_list('confidence_level_top').alias('top_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "                                            .alias('documents_topics_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+--------------------+\n",
      "|document_id_top|topic_id|confidence_level_top|\n",
      "+---------------+--------+--------------------+\n",
      "|        1595802|     140|          0.07311316|\n",
      "|        1595802|      16|         0.059416488|\n",
      "|        1595802|     143|         0.045420755|\n",
      "|        1595802|     170|          0.03886743|\n",
      "|        1524246|     113|           0.1964504|\n",
      "|        1524246|     260|          0.14287816|\n",
      "|        1524246|      92|          0.03315913|\n",
      "|        1524246|     168|        0.0140903415|\n",
      "|        1524246|      54|          0.00878222|\n",
      "|        1524246|     207|         0.008282372|\n",
      "+---------------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_topics_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_entities.csv 가져오기\n",
    "\n",
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\")  \\\n",
    "                .alias('documents_entities')\n",
    "\n",
    "# documents_entities_grouped_df 생성하기  \n",
    "# 데이터들을 document_id로 그룹화하고, 한 document_id에 해당하는 entity_id와 confidence_level을 각각 리스트로 담는다 \n",
    "# dummyDocumentsEntities 컬럼 추가 (entities 더미변수 생성)\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "                                            .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "                                                 F.collect_list('confidence_level_ent').alias('ent_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "                                            .alias('documents_entities_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+\n",
      "|document_id_ent|           entity_id|confidence_level_ent|\n",
      "+---------------+--------------------+--------------------+\n",
      "|        1524246|f9eec25663db4cd83...|          0.67286533|\n",
      "|        1524246|55ebcfbdaff1d6f60...|           0.3991137|\n",
      "|        1524246|839907a972930b17b...|          0.39209574|\n",
      "|        1524246|04d8f9a1ad48f126d...|          0.21399638|\n",
      "|        1617787|612a1d17685a498af...|          0.38619283|\n",
      "|        1617787|fb8c6cb0879e0de87...|          0.36411646|\n",
      "|        1617787|793c6a6cf386edb82...|          0.34916824|\n",
      "|        1617787|b525b84d5ed52a345...|          0.28700453|\n",
      "|        1617787|758cb9cb3014607cb...|          0.23795699|\n",
      "|        1617787|d523aaba6d3916f8b...|          0.23579852|\n",
      "+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_entities_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------------------+----------------------+\n",
      "|document_id_ent|      entity_id_list|ent_confidence_level_list|dummyDocumentsEntities|\n",
      "+---------------+--------------------+-------------------------+----------------------+\n",
      "|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|            463|[aaa0246895d43735...|              [0.6939791]|                     1|\n",
      "|            496|[0ffa5e294bd46905...|              [0.3608937]|                     1|\n",
      "|            833|[430da13f06eed7d5...|     [0.5932388, 0.240...|                     1|\n",
      "|           1088|[94101adfc2f6bccb...|              [0.9564353]|                     1|\n",
      "|           1580|[86b630e436676e43...|     [0.92001617, 0.44...|                     1|\n",
      "|           1645|[976e5e062b216f23...|     [0.66670954, 0.61...|                     1|\n",
      "|           1959|[806f6ef8cca7644d...|             [0.31478134]|                     1|\n",
      "|           2122|[bad3651e69ae382f...|     [0.6577534, 0.419...|                     1|\n",
      "|           2142|[75652dea0dbaeee4...|     [0.9794568, 0.693...|                     1|\n",
      "+---------------+--------------------+-------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_entities_grouped_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_meta, documents_categories_grouped, documents_topics_grouped, documents_entities_grouped를 조인한 documents_df 생성\n",
    "# ① documents_meta와 documents_categories_grouped를 조인 (key='document_id')\n",
    "# ② 위 테이블과 documents_topics_grouped를 조인 (key='document_id')\n",
    "# ③ 위 테이블과 documents_entities_grouped를 조인 (key='document_id')\n",
    "# cache() : 동일한 rdd를 재사용하고 싶을 때 사용하는 함수, persist 함수에서 저장 옵션을 memory_only로 한 것과 동일\n",
    "\n",
    "documents_df = documents_meta_df.join(documents_categories_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|document_id_cat|category_id_list|cat_confidence_level_list|dummyDocumentsCategory|document_id_top|       topic_id_list|top_confidence_level_list|dummyDocumentsTopics|document_id_ent|      entity_id_list|ent_confidence_level_list|dummyDocumentsEntities|\n",
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|            463|      166|         642|2008-12-09 19:00:00|                 1|            463|    [1513, 1808]|     [0.8932095, 0.067...|                     1|            463|[181, 292, 24, 25...|     [0.11870128, 0.05...|                   1|            463|[aaa0246895d43735...|              [0.6939791]|                     1|\n",
      "|            471|       83|         118|2008-12-08 00:00:00|                 1|            471|    [1504, 1609]|             [0.92, 0.07]|                     1|            471|[285, 238, 153, 193]|     [0.15588789, 0.04...|                   1|           null|                null|                     null|                  null|\n",
      "|            496|     2186|         184|2008-12-29 18:00:00|                 1|            496|    [1210, 1203]|             [0.92, 0.07]|                     1|            496|[244, 294, 196, 1...|     [0.18284231, 0.11...|                   1|            496|[0ffa5e294bd46905...|              [0.3608937]|                     1|\n",
      "|            833|      121|         655|2009-04-07 11:00:00|                 1|            833|    [1305, 2004]|             [0.92, 0.07]|                     1|            833|[294, 89, 174, 86...|     [0.11430275, 0.04...|                   1|            833|[430da13f06eed7d5...|     [0.5932388, 0.240...|                     1|\n",
      "|           1088|     2186|         184|2009-05-25 14:00:00|                 1|           1088|    [2006, 1210]|     [0.8364613, 0.063...|                     1|           1088|[107, 75, 153, 64...|     [0.10822894, 0.06...|                   1|           1088|[94101adfc2f6bccb...|              [0.9564353]|                     1|\n",
      "|           1238|     3233|         202|2007-05-10 00:00:00|                 1|           1238|    [1100, 1407]|     [0.34836665, 0.02...|                     1|           1238| [89, 221, 192, 236]|     [0.023348164, 0.0...|                   1|           null|                null|                     null|                  null|\n",
      "|           1342|      475|         937|2015-01-01 00:00:00|                 1|           1342|    [1408, 2004]|     [0.42835742, 0.03...|                     1|           1342|[271, 283, 181, 2...|     [0.0457309, 0.025...|                   1|           null|                null|                     null|                  null|\n",
      "|           1580|     2902|          56|2009-07-24 20:00:00|                 1|           1580|    [1403, 1402]|     [0.65625566, 0.04...|                     1|           1580|[8, 37, 136, 12, ...|     [0.08965496, 0.03...|                   1|           1580|[86b630e436676e43...|     [0.92001617, 0.44...|                     1|\n",
      "|           1591|     2589|         202|2009-06-01 00:00:00|                 1|           1591|    [1608, 1603]|     [0.35500738, 0.02...|                     1|           1591|               [260]|              [0.0628404]|                   1|           null|                null|                     null|                  null|\n",
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫째 행에서 evaluation = True 줬음\n",
    "# bucket에 저장한 validation set 가져오기\n",
    "# validation_set_df의 uuid_event의 unique 값을 users_to_profile로 저장\n",
    "# createOrReplaceTempView() : Spark는 Lazy evaluation이기 때문에 아직 실행 되기 전이다\n",
    "# uuid_event, document_id_promo 두 컬럼의 unique 값을 추출해서 validation_users_docs_to_ignore로 저장\n",
    "# document_id_promo는 document_id와 다른 광고 랜딩페이지이기 때문에 docs_to_ignore로 따로 저장된 것 같다\n",
    "# evaluation이 false면 test set을 생성하기 위한 스키마랑 테이블을 가져와서 test set을 생성\n",
    "\n",
    "if evaluation:\n",
    "    validation_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "                    .alias('validation_set')        \n",
    "    \n",
    "    validation_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')    \n",
    "    validation_set_df.select('uuid_event','document_id_promo').distinct().createOrReplaceTempView('validation_users_docs_to_ignore')\n",
    "    \n",
    "else:\n",
    "    events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "    events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"events.csv\") \\\n",
    "                    .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                    .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                    .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                    .alias('events')\n",
    "\n",
    "    events_df.createOrReplaceTempView('events')\n",
    "\n",
    "\n",
    "    promoted_content_schema = StructType(\n",
    "                        [StructField(\"ad_id\", IntegerType(), True),\n",
    "                        StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                        StructField(\"campaign_id\", IntegerType(), True),\n",
    "                        StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                        )\n",
    "\n",
    "    promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                    .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content')\n",
    "    \n",
    "    # clicks_test.csv 가져오기\n",
    "    clicks_test_schema = StructType(\n",
    "                        [StructField(\"display_id\", IntegerType(), True),\n",
    "                        StructField(\"ad_id\", IntegerType(), True)]\n",
    "                        )\n",
    "    \n",
    "    # dummyClicksTest 컬럼 추가 (display_id - ad_id 더미변수 생성)\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)).alias('clicks_test')\n",
    "    \n",
    "    # clicks_test와 promoted_content를 조인 (key='ad_id')\n",
    "    # 위의 테이블과 events를 조인 (key='display_id')\n",
    "    # test_set_df를 생성\n",
    "    test_set_df = clicks_test_df.join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                                .join(events_df, on='display_id', how='left')\n",
    "    \n",
    "    # test_set_df에서 uuid_event의 unique값을 추출하여 users_to_profile에 저장\n",
    "    test_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')\n",
    "    \n",
    "    # test_set_df에서 uuid_event, document_id_promo, timestamp_event의 unique 값을 추출해서 test_users_docs_timestamp_to_ignore로 저장\n",
    "    test_set_df.select('uuid_event', 'document_id_promo', 'timestamp_event').distinct().createOrReplaceTempView('test_users_docs_timestamp_to_ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promoted_content_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicks_test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_views.csv 가져오기\n",
    "# day_pv 컬럼 추가 : view라는 액션이 일어난 timestamp를, 첫날을 기준으로 경과한 일자(숫자)로 반환한 값을 갖는 컬럼 \n",
    "\n",
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "# Google Storage에 저장된 page_views 파일 로드\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(\"gs://upload-bigquery180927/page_views.csv\") \\\n",
    "                .withColumn('day_pv', truncate_day_from_timestamp_udf('timestamp_pv')) \\\n",
    "                .alias('page_views')             \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_id가 document_id_promo랑 일치하면 제외 (document_id_promo는 document_id와 달리 랜딩페이지이기 때문에 page view가 아님)\n",
    "# (evaluation이 false면) 다음 조건 추가 : page_views의 timestamp가 event의 timestamp보다 나중인 것 제외 (click이 view보다 앞선 데이터는 신뢰도 낮음)\n",
    "\n",
    "additional_filter = ''\n",
    "if evaluation:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM validation_users_docs_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv)\n",
    "                        '''\n",
    "else:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM test_users_docs_timestamp_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv\n",
    "                                                     AND p.timestamp_pv >= timestamp_event)\n",
    "                        '''\n",
    "\n",
    "# users_to_profile : validation_set_df의 uuid_event의 unique 값\n",
    "# page_views_train_df 생성\n",
    "# ① page_views에서 uuid가 users_to_profile의 uuid와 일치하는 모든 데이터 가져옴\n",
    "# ② documents_df(meta, cate, topics, entities 조인한 것)와 조인 (key='document_id')\n",
    "# ③ filter 부분 해석 불가\n",
    "\n",
    "page_views_train_df = spark.sql('''SELECT * FROM page_views p \n",
    "                                    WHERE EXISTS (SELECT uuid_event FROM users_to_profile\n",
    "                                                 WHERE uuid_event = p.uuid_pv)                                     \n",
    "                                '''+ additional_filter\n",
    "                               ).alias('views') \\\n",
    "                         .join(documents_df, on=F.col(\"document_id_pv\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                         .filter('dummyDocumentsEntities is not null OR dummyDocumentsTopics is not null OR dummyDocumentsCategory is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------+-----------+---------------+-----------------+------+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|       uuid_pv|document_id_pv|timestamp_pv|platform_pv|geo_location_pv|traffic_source_pv|day_pv|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|document_id_cat|category_id_list|cat_confidence_level_list|dummyDocumentsCategory|document_id_top|       topic_id_list|top_confidence_level_list|dummyDocumentsTopics|document_id_ent|      entity_id_list|ent_confidence_level_list|dummyDocumentsEntities|\n",
      "+--------------+--------------+------------+-----------+---------------+-----------------+------+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|6fcc3f25120625|           148|   489282978|          2|      US>MA>521|                2|     5|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|27a20b334ffe51|           148|  1004798913|          2|          CA>ON|                2|    11|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|1d6b86fdbe5835|           148|  1264590600|          2|      US>AZ>753|                1|    14|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|3ff68bb08d04fe|           148|  1204616988|          2|      US>CA>803|                2|    13|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|e0d4f60260a2e4|           148|  1050571285|          2|      US>GA>524|                1|    12|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|b9e58af67d8071|           148|  1002216814|          2|      US>CO>752|                2|    11|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|537070ac5e3f31|           148|   837010232|          2|      US>PA>504|                2|     9|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|ff6e4010334bf9|           148|   998381923|          2|      US>SC>570|                1|    11|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|4bb37e6a5c24d9|           148|  1089760375|          1|      US>IL>602|                2|    12|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|634c4b008c5334|           463|   137437385|          1|      US>GA>524|                1|     1|            463|      166|         642|2008-12-09 19:00:00|                 1|            463|    [1513, 1808]|     [0.8932095, 0.067...|                     1|            463|[181, 292, 24, 25...|     [0.11870128, 0.05...|                   1|            463|[aaa0246895d43735...|              [0.6939791]|                     1|\n",
      "+--------------+--------------+------------+-----------+---------------+-----------------+------+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_views_train_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document_meta의 데이터 개수 카운트\n",
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_docs_counts = documents_categories_df.groupBy('category_id').count().rdd.collectAsMap()\n",
    "len(categories_docs_counts)\n",
    "\n",
    "# collectAsMap() : return the results for paired RDD as Map collection\n",
    "# categories_docs_counts는 category_id로 묶고, category_id와 그 개수를 pair 형태로 갖는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'\n",
    "    \n",
    "# evaluation이 true면, df_filenames_suffix의 값을 _eval로 준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(categories_docs_counts, output)\n",
    "\n",
    "# pickle : 파이썬에서 리스트, 클래스 같은 텍스트 이외의 자료형을 파일로 저장하기 위하여 사용하는 모듈    \n",
    "# pickle.load(파일)을 통해서 파일 내용을 읽어오려면 pickle.dump를 사용해서 데이터를 입력한 파일이어야 한다\n",
    "# pickle.dump(데이터, 파일) : 데이터를 파일에 저장한다\n",
    "# 'wb' = write and binary\n",
    "# categories_docs_counts를 output 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle을 사용한 데이터 save와 load 예시\n",
    "\n",
    "# save\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_docs_counts = documents_topics_df.groupBy('topic_id').count().rdd.collectAsMap()\n",
    "len(topics_docs_counts)\n",
    "\n",
    "# topics_docs_counts는 topic_id로 묶고, topic_id와 그 개수를 pair 형태로 갖는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topics_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(topics_docs_counts, output)\n",
    "    \n",
    "# topics_docs_counts를 output 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_docs_counts = documents_entities_df.groupBy('entity_id').count().rdd.collectAsMap()\n",
    "len(entities_docs_counts)\n",
    "\n",
    "# entities_docs_counts는 entity_id로 묶고, entity_id와 그 개수를 pair 형태로 갖는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(entities_docs_counts, output)\n",
    "    \n",
    "# entities_docs_counts를 output 폴더에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_null_to_minus_one_udf -> int가 null값이면 -1을 반환하는 함수\n",
    "# 아래 3개 함수 -> 각각 int, float, str 타입인 리스트 형태 데이터가 null 값이면 빈 리스트 반환하는 함수\n",
    "\n",
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else -1, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_views_by_user_df 생성\n",
    "# ① page_views에서 uuid_pv, document_id_pv는 그대로 가져오고, 나머지는 위 네 함수 적용해서 가져옴\n",
    "# ② uuid_pv로 그룹핑\n",
    "# ③ uuid_pv에 해당하는 각 컬럼들의 정보를 하나의 리스트 안에 넣는다 -> F.collect_list()\n",
    "\n",
    "page_views_by_user_df = page_views_train_df.select(\n",
    "                           'uuid_pv', \n",
    "                           'document_id_pv', \n",
    "                           int_null_to_minus_one_udf('timestamp_pv').alias('timestamp_pv'), \n",
    "                           int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('cat_confidence_level_list').alias('cat_confidence_level_list'), \n",
    "                           int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('top_confidence_level_list').alias('top_confidence_level_list'), \n",
    "                           str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('ent_confidence_level_list').alias('ent_confidence_level_list')) \\\n",
    "                    .groupBy('uuid_pv') \\\n",
    "                    .agg(F.collect_list('document_id_pv').alias('document_id_pv_list'),\n",
    "                         F.collect_list('timestamp_pv').alias('timestamp_pv_list'),\n",
    "                         F.collect_list('category_id_list').alias('category_id_lists'),\n",
    "                         F.collect_list('cat_confidence_level_list').alias('cat_confidence_level_lists'),\n",
    "                         F.collect_list('topic_id_list').alias('topic_id_lists'),\n",
    "                         F.collect_list('top_confidence_level_list').alias('top_confidence_level_lists'),\n",
    "                         F.collect_list('entity_id_list').alias('entity_id_lists'),\n",
    "                         F.collect_list('ent_confidence_level_list').alias('ent_confidence_level_lists')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# get_user_aspects() 함수 생성\n",
    "\n",
    "def get_user_aspects(docs_aspects, aspect_docs_counts):\n",
    "    docs_aspects_merged_lists = defaultdict(list)\n",
    "    \n",
    "    for doc_aspects in docs_aspects:\n",
    "        for key in doc_aspects.keys():\n",
    "            docs_aspects_merged_lists[key].append(doc_aspects[key])\n",
    "        \n",
    "    docs_aspects_stats = {}\n",
    "    for key in docs_aspects_merged_lists.keys():\n",
    "        aspect_list = docs_aspects_merged_lists[key]\n",
    "        tf = len(aspect_list)\n",
    "        idf = math.log(documents_total / float(aspect_docs_counts[key]))\n",
    "        \n",
    "        confid_mean = sum(aspect_list) / float(len(aspect_list))\n",
    "        docs_aspects_stats[key] = [tf*idf, confid_mean]\n",
    "        \n",
    "    return docs_aspects_stats\n",
    "\n",
    "# generate_user_profile() 함수 생성\n",
    "\n",
    "def generate_user_profile(docs_aspects_list, docs_aspects_confidence_list, aspect_docs_counts):    \n",
    "    docs_aspects = []\n",
    "    for doc_aspects_list, doc_aspects_confidence_list in zip(docs_aspects_list, docs_aspects_confidence_list):\n",
    "        doc_aspects = dict(zip(doc_aspects_list, doc_aspects_confidence_list))\n",
    "        docs_aspects.append(doc_aspects)\n",
    "        \n",
    "    user_aspects = get_user_aspects(docs_aspects, aspect_docs_counts)\n",
    "    return user_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_list_len_udf() : input값의 길이를 반환하는 함수\n",
    "\n",
    "get_list_len_udf = F.udf(lambda docs_list: len(docs_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_categories_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            categories_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_topics_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            topics_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_entities_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            entities_docs_counts), \n",
    "                                          MapType(StringType(),\n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_profile_df = page_views_by_user_df \\\n",
    "                                 .withColumn('views', get_list_len_udf('document_id_pv_list')) \\\n",
    "                                 .withColumn('categories', \n",
    "                                             generate_categories_user_profile_map_udf('category_id_lists', \n",
    "                                                                   'cat_confidence_level_lists')) \\\n",
    "                                 .withColumn('topics', \n",
    "                                             generate_topics_user_profile_map_udf('topic_id_lists', \n",
    "                                                                               'top_confidence_level_lists')) \\\n",
    "                                 .withColumn('entities', \n",
    "                                             generate_entities_user_profile_map_udf('entity_id_lists', \n",
    "                                                                               'ent_confidence_level_lists')) \\\n",
    "                                 .select(F.col('uuid_pv').alias('uuid'),\n",
    "                                         F.col('document_id_pv_list').alias('doc_ids'),\n",
    "                                         'views',\n",
    "                                         'categories', 'topics', 'entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|          uuid|             doc_ids|views|          categories|              topics|            entities|\n",
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|100013af048bbf|[2442888, 2434217...|   46|Map(1205 -> Wrapp...|Map(5 -> WrappedA...|Map(14a7d4c4ebcc6...|\n",
      "|100163b35102c4|[2516821, 2356657...|   12|Map(1907 -> Wrapp...|Map(174 -> Wrappe...|Map(6904a5638b5cf...|\n",
      "|1003370a1c2d0f|[1521640, 1685708...|    4|Map(1808 -> Wrapp...|Map(69 -> Wrapped...|Map(531cadf46e145...|\n",
      "|100659017f177b|            [429642]|    1|Map(1510 -> Wrapp...|Map(296 -> Wrappe...|               Map()|\n",
      "|100aa12f880396|[2504276, 1792136...|    3|Map(1408 -> Wrapp...|Map(265 -> Wrappe...|Map(b366917165b76...|\n",
      "|101324634e39b0|  [2672785, 2690250]|    2|Map(1210 -> Wrapp...|Map(20 -> Wrapped...|Map(14a7d4c4ebcc6...|\n",
      "|101487b48a7780|[1113514, 1184645...|   44|Map(1406 -> Wrapp...|Map(138 -> Wrappe...|Map(5096ae94f0e6d...|\n",
      "|1014e25bc11b1b|[1809496, 1809496...|    7|Map(1406 -> Wrapp...|Map(249 -> Wrappe...|Map(b97df21a91594...|\n",
      "|101bd04e3e07d7|           [2111911]|    1|Map(1100 -> Wrapp...|Map(232 -> Wrappe...|Map(d78596b7a5bdc...|\n",
      "|101dae0b4172f6|[2418906, 2257944...|    4|Map(1708 -> Wrapp...|Map(10 -> Wrapped...|Map(6de5d563d57c4...|\n",
      "+--------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_profile_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'\n",
    "\n",
    "users_profile_df.write.parquet(OUTPUT_BUCKET_FOLDER+table_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Elapsed min: ', 0.5069252494308684)\n"
     ]
    }
   ],
   "source": [
    "finish_time = time.time()\n",
    "print(\"Elapsed min: \", (finish_time-start_time)/60/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
