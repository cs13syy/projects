{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 작업은, 코드를 입력하고 실행하면 바로 실행되는 것이 아니다\n",
    "# 지연 연산(lazy evaluation)을 사용해서 기초 데이터에 적용될 변환 연산을 기억하고 있고 동작(Action)이 실행될 때 한 번에 실행된다\n",
    "# 이런 과정을 통해 스파크가 자동으로 최적화 및 장애나 느리게 작업하는 일꾼을 깔끔하게 처리한다\n",
    "\n",
    "evaluation = True\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://capstone-01/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://capstone-01/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 함수 udf\n",
    "# timestamp를 넣으면 일자(숫자) 반환\n",
    "# 예를 들어, events.csv의 timestamp를 이 함수에 넣으면, ad를 클릭한 날이 기준일로부터 몇 일인지 반환 (0~12) \n",
    "\n",
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip 함수로 데이터의 양쪽 공백을 지우고, 앞에서 2자만 떼어내서 반환하는 함수 (데이터가 None이면 '' 반환)\n",
    "\n",
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_meta.csv 가져오기\n",
    "# dummyDocumentsMeta 컬럼 추가\n",
    "# withColumn('컬럼명', F.lit(1)) -> 더미변수 만드는 함수, 하나의 컬럼으로 추가됨\n",
    "# F.lit() : Creates a Column of literal value, lit 함수는 상수 컬럼을 만들 때 사용한다\n",
    "# you have to use lit if you want to access any of the pyspark.sql.Column methods treating standard Python scalar as a constant column\n",
    "\n",
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+-------------------+------------------+\n",
      "|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|\n",
      "+---------------+---------+------------+-------------------+------------------+\n",
      "|        1595802|        1|         603|2016-06-05 00:00:00|                 1|\n",
      "|        1524246|        1|         603|2016-05-26 11:00:00|                 1|\n",
      "|        1617787|        1|         603|2016-05-27 00:00:00|                 1|\n",
      "|        1615583|        1|         603|2016-06-07 00:00:00|                 1|\n",
      "|        1615460|        1|         603|2016-06-20 00:00:00|                 1|\n",
      "|        1615354|        1|         603|2016-06-10 00:00:00|                 1|\n",
      "|        1614611|        1|         603|2016-06-05 13:00:00|                 1|\n",
      "|        1614235|        1|         603|2016-06-09 00:00:00|                 1|\n",
      "|        1614225|        1|         603|2016-06-09 00:00:00|                 1|\n",
      "|        1488264|        1|         603|2016-05-23 13:00:00|                 1|\n",
      "+---------------+---------+------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_meta_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_categories.csv 가져오기 \n",
    "\n",
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories')\n",
    "\n",
    "# documents_categories_grouped_df 생성하기\n",
    "# agg 함수에는 원하는 기능과 컬럼을 전달한다\n",
    "# 데이터들을 document_id로 그룹화하고, 한 document_id에 해당하는 category_id와 confidence_level을 각각 리스트로 담는다  \n",
    "# F.collect_list() : 주어진 컬럼의 모든 값을 수집하여 하나의 리스트로 만듦\n",
    "# dummyDocumentsCategory 컬럼 추가 (category 더미변수 생성)\n",
    "\n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "                                                 F.collect_list('confidence_level_cat').alias('cat_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "                                            .alias('documents_categories_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------------+\n",
      "|document_id_cat|category_id|confidence_level_cat|\n",
      "+---------------+-----------+--------------------+\n",
      "|        1595802|       1611|                0.92|\n",
      "|        1595802|       1610|                0.07|\n",
      "|        1524246|       1807|                0.92|\n",
      "|        1524246|       1608|                0.07|\n",
      "|        1617787|       1807|                0.92|\n",
      "|        1617787|       1608|                0.07|\n",
      "|        1615583|       1305|                0.92|\n",
      "|        1615583|       1806|                0.07|\n",
      "|        1615460|       1613|           0.5406464|\n",
      "|        1615460|       1603|          0.04113614|\n",
      "+---------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_categories_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_topics.csv 가져오기 \n",
    "\n",
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics')\n",
    "    \n",
    "# documents_topics_grouped_df 생성하기  \n",
    "# 데이터들을 document_id로 그룹화하고, 한 document_id에 해당하는 topic_id와 confidence_level을 각각 리스트로 담는다 \n",
    "# dummyDocumentsTopics 컬럼 추가 (topics 더미변수 생성)\n",
    "\n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "                                                 F.collect_list('confidence_level_top').alias('top_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "                                            .alias('documents_topics_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+--------------------+\n",
      "|document_id_top|topic_id|confidence_level_top|\n",
      "+---------------+--------+--------------------+\n",
      "|        1595802|     140|          0.07311316|\n",
      "|        1595802|      16|         0.059416488|\n",
      "|        1595802|     143|         0.045420755|\n",
      "|        1595802|     170|          0.03886743|\n",
      "|        1524246|     113|           0.1964504|\n",
      "|        1524246|     260|          0.14287816|\n",
      "|        1524246|      92|          0.03315913|\n",
      "|        1524246|     168|        0.0140903415|\n",
      "|        1524246|      54|          0.00878222|\n",
      "|        1524246|     207|         0.008282372|\n",
      "+---------------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_topics_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_entities.csv 가져오기\n",
    "\n",
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\")  \\\n",
    "                .alias('documents_entities')\n",
    "\n",
    "# documents_entities_grouped_df 생성하기  \n",
    "# 데이터들을 document_id로 그룹화하고, 한 document_id에 해당하는 entity_id와 confidence_level을 각각 리스트로 담는다 \n",
    "# dummyDocumentsEntities 컬럼 추가 (entities 더미변수 생성)\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "                                            .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "                                                 F.collect_list('confidence_level_ent').alias('ent_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "                                            .alias('documents_entities_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+\n",
      "|document_id_ent|           entity_id|confidence_level_ent|\n",
      "+---------------+--------------------+--------------------+\n",
      "|        1524246|f9eec25663db4cd83...|          0.67286533|\n",
      "|        1524246|55ebcfbdaff1d6f60...|           0.3991137|\n",
      "|        1524246|839907a972930b17b...|          0.39209574|\n",
      "|        1524246|04d8f9a1ad48f126d...|          0.21399638|\n",
      "|        1617787|612a1d17685a498af...|          0.38619283|\n",
      "|        1617787|fb8c6cb0879e0de87...|          0.36411646|\n",
      "|        1617787|793c6a6cf386edb82...|          0.34916824|\n",
      "|        1617787|b525b84d5ed52a345...|          0.28700453|\n",
      "|        1617787|758cb9cb3014607cb...|          0.23795699|\n",
      "|        1617787|d523aaba6d3916f8b...|          0.23579852|\n",
      "+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_entities_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------------------+----------------------+\n",
      "|document_id_ent|      entity_id_list|ent_confidence_level_list|dummyDocumentsEntities|\n",
      "+---------------+--------------------+-------------------------+----------------------+\n",
      "|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|            463|[aaa0246895d43735...|              [0.6939791]|                     1|\n",
      "|            496|[0ffa5e294bd46905...|              [0.3608937]|                     1|\n",
      "|            833|[430da13f06eed7d5...|     [0.5932388, 0.240...|                     1|\n",
      "|           1088|[94101adfc2f6bccb...|              [0.9564353]|                     1|\n",
      "|           1580|[86b630e436676e43...|     [0.92001617, 0.44...|                     1|\n",
      "|           1645|[976e5e062b216f23...|     [0.66670954, 0.61...|                     1|\n",
      "|           1959|[806f6ef8cca7644d...|             [0.31478134]|                     1|\n",
      "|           2122|[bad3651e69ae382f...|     [0.6577534, 0.419...|                     1|\n",
      "|           2142|[75652dea0dbaeee4...|     [0.9794568, 0.693...|                     1|\n",
      "+---------------+--------------------+-------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_entities_grouped_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents_meta, documents_categories_grouped, documents_topics_grouped, documents_entities_grouped를 조인한 documents_df 생성\n",
    "# ① documents_meta와 documents_categories_grouped를 조인 (key='document_id')\n",
    "# ② 위 테이블과 documents_topics_grouped를 조인 (key='document_id')\n",
    "# ③ 위 테이블과 documents_entities_grouped를 조인 (key='document_id')\n",
    "# cache() : 동일한 rdd를 재사용하고 싶을 때 사용하는 함수, persist 함수에서 저장 옵션을 memory_only로 한 것과 동일\n",
    "\n",
    "documents_df = documents_meta_df.join(documents_categories_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|document_id_doc|source_id|publisher_id|       publish_time|dummyDocumentsMeta|document_id_cat|category_id_list|cat_confidence_level_list|dummyDocumentsCategory|document_id_top|       topic_id_list|top_confidence_level_list|dummyDocumentsTopics|document_id_ent|      entity_id_list|ent_confidence_level_list|dummyDocumentsEntities|\n",
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "|            148|     1787|         118|2008-07-01 00:00:00|                 1|            148|    [1403, 1702]|             [0.92, 0.07]|                     1|            148|[153, 140, 8, 172...|     [0.07523697, 0.07...|                   1|            148|[e1c74838563ef5d2...|     [0.6320258, 0.404...|                     1|\n",
      "|            463|      166|         642|2008-12-09 19:00:00|                 1|            463|    [1513, 1808]|     [0.8932095, 0.067...|                     1|            463|[181, 292, 24, 25...|     [0.11870128, 0.05...|                   1|            463|[aaa0246895d43735...|              [0.6939791]|                     1|\n",
      "|            471|       83|         118|2008-12-08 00:00:00|                 1|            471|    [1504, 1609]|             [0.92, 0.07]|                     1|            471|[285, 238, 153, 193]|     [0.15588789, 0.04...|                   1|           null|                null|                     null|                  null|\n",
      "|            496|     2186|         184|2008-12-29 18:00:00|                 1|            496|    [1210, 1203]|             [0.92, 0.07]|                     1|            496|[244, 294, 196, 1...|     [0.18284231, 0.11...|                   1|            496|[0ffa5e294bd46905...|              [0.3608937]|                     1|\n",
      "|            833|      121|         655|2009-04-07 11:00:00|                 1|            833|    [1305, 2004]|             [0.92, 0.07]|                     1|            833|[294, 89, 174, 86...|     [0.11430275, 0.04...|                   1|            833|[430da13f06eed7d5...|     [0.5932388, 0.240...|                     1|\n",
      "|           1088|     2186|         184|2009-05-25 14:00:00|                 1|           1088|    [2006, 1210]|     [0.8364613, 0.063...|                     1|           1088|[107, 75, 153, 64...|     [0.10822894, 0.06...|                   1|           1088|[94101adfc2f6bccb...|              [0.9564353]|                     1|\n",
      "|           1238|     3233|         202|2007-05-10 00:00:00|                 1|           1238|    [1100, 1407]|     [0.34836665, 0.02...|                     1|           1238| [89, 221, 192, 236]|     [0.023348164, 0.0...|                   1|           null|                null|                     null|                  null|\n",
      "|           1342|      475|         937|2015-01-01 00:00:00|                 1|           1342|    [1408, 2004]|     [0.42835742, 0.03...|                     1|           1342|[271, 283, 181, 2...|     [0.0457309, 0.025...|                   1|           null|                null|                     null|                  null|\n",
      "|           1580|     2902|          56|2009-07-24 20:00:00|                 1|           1580|    [1403, 1402]|     [0.65625566, 0.04...|                     1|           1580|[8, 37, 136, 12, ...|     [0.08965496, 0.03...|                   1|           1580|[86b630e436676e43...|     [0.92001617, 0.44...|                     1|\n",
      "|           1591|     2589|         202|2009-06-01 00:00:00|                 1|           1591|    [1608, 1603]|     [0.35500738, 0.02...|                     1|           1591|               [260]|              [0.0628404]|                   1|           null|                null|                     null|                  null|\n",
      "+---------------+---------+------------+-------------------+------------------+---------------+----------------+-------------------------+----------------------+---------------+--------------------+-------------------------+--------------------+---------------+--------------------+-------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫째 행에서 evaluation = True 줬음\n",
    "# bucket에 저장한 validation set 가져오기\n",
    "# validation_set_df의 uuid_event의 unique 값을 users_to_profile로 저장\n",
    "# createOrReplaceTempView() : Spark는 Lazy evaluation이기 때문에 아직 실행 되기 전이다\n",
    "# uuid_event, document_id_promo 두 컬럼의 unique 값을 추출해서 validation_users_docs_to_ignore로 저장\n",
    "# document_id_promo는 document_id와 다른 광고 랜딩페이지이기 때문에 docs_to_ignore로 따로 저장된 것 같다\n",
    "# evaluation이 false면 test set을 생성하기 위한 스키마랑 테이블을 가져와서 test set을 생성\n",
    "\n",
    "if evaluation:\n",
    "    validation_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "                    .alias('validation_set')        \n",
    "    \n",
    "    validation_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')    \n",
    "    validation_set_df.select('uuid_event','document_id_promo').distinct().createOrReplaceTempView('validation_users_docs_to_ignore')\n",
    "    \n",
    "else:\n",
    "    events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "    events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"events.csv\") \\\n",
    "                    .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                    .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                    .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                    .alias('events')\n",
    "\n",
    "    events_df.createOrReplaceTempView('events')\n",
    "\n",
    "\n",
    "    promoted_content_schema = StructType(\n",
    "                        [StructField(\"ad_id\", IntegerType(), True),\n",
    "                        StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                        StructField(\"campaign_id\", IntegerType(), True),\n",
    "                        StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                        )\n",
    "\n",
    "    promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                    .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content')\n",
    "    \n",
    "    # clicks_test.csv 가져오기\n",
    "    clicks_test_schema = StructType(\n",
    "                        [StructField(\"display_id\", IntegerType(), True),\n",
    "                        StructField(\"ad_id\", IntegerType(), True)]\n",
    "                        )\n",
    "    \n",
    "    # dummyClicksTest 컬럼 추가 (display_id - ad_id 더미변수 생성)\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)).alias('clicks_test')\n",
    "    \n",
    "    # clicks_test와 promoted_content를 조인 (key='ad_id')\n",
    "    # 위의 테이블과 events를 조인 (key='display_id')\n",
    "    # test_set_df를 생성\n",
    "    test_set_df = clicks_test_df.join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                                .join(events_df, on='display_id', how='left')\n",
    "    \n",
    "    # test_set_df에서 uuid_event의 unique값을 추출하여 users_to_profile에 저장\n",
    "    test_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')\n",
    "    \n",
    "    # test_set_df에서 uuid_event, document_id_promo, timestamp_event의 unique 값을 추출해서 test_users_docs_timestamp_to_ignore로 저장\n",
    "    test_set_df.select('uuid_event', 'document_id_promo', 'timestamp_event').distinct().createOrReplaceTempView('test_users_docs_timestamp_to_ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promoted_content_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicks_test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_views.csv 가져오기\n",
    "# day_pv 컬럼 추가 : view라는 액션이 일어난 timestamp를, 첫날을 기준으로 경과한 일자(숫자)로 반환한 값을 갖는 컬럼 \n",
    "\n",
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "# Google Storage에 저장된 page_views 파일 로드\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(\"gs://upload-bigquery180927/page_views.csv\") \\\n",
    "                .withColumn('day_pv', truncate_day_from_timestamp_udf('timestamp_pv')) \\\n",
    "                .alias('page_views')             \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_id가 document_id_promo랑 일치하면 제외 (document_id_promo는 document_id와 달리 랜딩페이지이기 때문에 page view가 아님)\n",
    "# (evaluation이 false면) 다음 조건 추가 : page_views의 timestamp가 event의 timestamp보다 나중인 것 제외 (click이 view보다 앞선 데이터는 신뢰도 낮음)\n",
    "\n",
    "additional_filter = ''\n",
    "if evaluation:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM validation_users_docs_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv)\n",
    "                        '''\n",
    "else:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM test_users_docs_timestamp_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv\n",
    "                                                     AND p.timestamp_pv >= timestamp_event)\n",
    "                        '''\n",
    "\n",
    "# users_to_profile : validation_set_df의 uuid_event의 unique 값\n",
    "# page_views_train_df 생성\n",
    "# ① page_views에서 uuid가 users_to_profile의 uuid와 일치하는 모든 데이터 가져옴\n",
    "# ② documents_df(meta, cate, topics, entities 조인한 것)와 조인 (key='document_id')\n",
    "# ③ filter 부분 해석 불가\n",
    "\n",
    "page_views_train_df = spark.sql('''SELECT * FROM page_views p \n",
    "                                    WHERE EXISTS (SELECT uuid_event FROM users_to_profile\n",
    "                                                 WHERE uuid_event = p.uuid_pv)                                     \n",
    "                                '''+ additional_filter\n",
    "                               ).alias('views') \\\n",
    "                         .join(documents_df, on=F.col(\"document_id_pv\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                         .filter('dummyDocumentsEntities is not null OR dummyDocumentsTopics is not null OR dummyDocumentsCategory is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_views_train_df.show(10) # not working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document_meta의 데이터 개수 카운트\n",
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_docs_counts = documents_categories_df.groupBy('category_id').count().rdd.collectAsMap()\n",
    "len(categories_docs_counts)\n",
    "\n",
    "# collectAsMap() : return the results for paired RDD as Map collection\n",
    "# categories_docs_counts는 category_id로 묶고, category_id와 그 개수를 pair 형태로 갖는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'\n",
    "    \n",
    "# evaluation이 true면, df_filenames_suffix의 값을 _eval로 준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(categories_docs_counts, output)\n",
    "\n",
    "# pickle : 파이썬에서 리스트, 클래스 같은 텍스트 이외의 자료형을 파일로 저장하기 위하여 사용하는 모듈    \n",
    "# pickle.load(파일)을 통해서 파일 내용을 읽어오려면 pickle.dump를 사용해서 데이터를 입력한 파일이어야 한다\n",
    "# pickle.dump(데이터, 파일) : 데이터를 파일에 저장한다\n",
    "# 'wb' = write and binary\n",
    "# categories_docs_counts를 output 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle을 사용한 데이터 save와 load 예시\n",
    "\n",
    "# save\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_docs_counts = documents_topics_df.groupBy('topic_id').count().rdd.collectAsMap()\n",
    "len(topics_docs_counts)\n",
    "\n",
    "# topics_docs_counts는 topic_id로 묶고, topic_id와 그 개수를 pair 형태로 갖는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('topics_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(topics_docs_counts, output)\n",
    "    \n",
    "# topics_docs_counts를 output 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_docs_counts = documents_entities_df.groupBy('entity_id').count().rdd.collectAsMap()\n",
    "len(entities_docs_counts)\n",
    "\n",
    "# entities_docs_counts는 entity_id로 묶고, entity_id와 그 개수를 pair 형태로 갖는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(entities_docs_counts, output)\n",
    "    \n",
    "# entities_docs_counts를 output 폴더에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_null_to_minus_one_udf -> int가 null값이면 -1을 반환하는 함수\n",
    "# 아래 3개 함수 -> 각각 int, float, str 타입인 리스트 형태 데이터가 null 값이면 빈 리스트 반환하는 함수\n",
    "\n",
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else -1, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_views_by_user_df 생성\n",
    "# ① page_views에서 uuid_pv, document_id_pv는 그대로 가져오고, 나머지는 위 네 함수 적용해서 가져옴\n",
    "# ② uuid_pv로 그룹핑\n",
    "# ③ uuid_pv에 해당하는 각 컬럼들의 정보를 하나의 리스트 안에 넣는다 -> F.collect_list()\n",
    "\n",
    "page_views_by_user_df = page_views_train_df.select(\n",
    "                           'uuid_pv', \n",
    "                           'document_id_pv', \n",
    "                           int_null_to_minus_one_udf('timestamp_pv').alias('timestamp_pv'), \n",
    "                           int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('cat_confidence_level_list').alias('cat_confidence_level_list'), \n",
    "                           int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('top_confidence_level_list').alias('top_confidence_level_list'), \n",
    "                           str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('ent_confidence_level_list').alias('ent_confidence_level_list')) \\\n",
    "                    .groupBy('uuid_pv') \\\n",
    "                    .agg(F.collect_list('document_id_pv').alias('document_id_pv_list'),\n",
    "                         F.collect_list('timestamp_pv').alias('timestamp_pv_list'),\n",
    "                         F.collect_list('category_id_list').alias('category_id_lists'),\n",
    "                         F.collect_list('cat_confidence_level_list').alias('cat_confidence_level_lists'),\n",
    "                         F.collect_list('topic_id_list').alias('topic_id_lists'),\n",
    "                         F.collect_list('top_confidence_level_list').alias('top_confidence_level_lists'),\n",
    "                         F.collect_list('entity_id_list').alias('entity_id_lists'),\n",
    "                         F.collect_list('ent_confidence_level_list').alias('ent_confidence_level_lists')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# get_user_aspects() 함수 생성\n",
    "\n",
    "def get_user_aspects(docs_aspects, aspect_docs_counts):\n",
    "    docs_aspects_merged_lists = defaultdict(list)\n",
    "    \n",
    "    for doc_aspects in docs_aspects:\n",
    "        for key in doc_aspects.keys():\n",
    "            docs_aspects_merged_lists[key].append(doc_aspects[key])\n",
    "        \n",
    "    docs_aspects_stats = {}\n",
    "    for key in docs_aspects_merged_lists.keys():\n",
    "        aspect_list = docs_aspects_merged_lists[key]\n",
    "        tf = len(aspect_list)\n",
    "        idf = math.log(documents_total / float(aspect_docs_counts[key]))\n",
    "        \n",
    "        confid_mean = sum(aspect_list) / float(len(aspect_list))\n",
    "        docs_aspects_stats[key] = [tf*idf, confid_mean]\n",
    "        \n",
    "    return docs_aspects_stats\n",
    "\n",
    "# generate_user_profile() 함수 생성\n",
    "\n",
    "def generate_user_profile(docs_aspects_list, docs_aspects_confidence_list, aspect_docs_counts):    \n",
    "    docs_aspects = []\n",
    "    for doc_aspects_list, doc_aspects_confidence_list in zip(docs_aspects_list, docs_aspects_confidence_list):\n",
    "        doc_aspects = dict(zip(doc_aspects_list, doc_aspects_confidence_list))\n",
    "        docs_aspects.append(doc_aspects)\n",
    "        \n",
    "    user_aspects = get_user_aspects(docs_aspects, aspect_docs_counts)\n",
    "    return user_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_list_len_udf() : input값의 길이를 반환하는 함수\n",
    "\n",
    "get_list_len_udf = F.udf(lambda docs_list: len(docs_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_categories_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            categories_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_topics_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            topics_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_entities_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            entities_docs_counts), \n",
    "                                          MapType(StringType(),\n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_profile_df = page_views_by_user_df \\\n",
    "                                 .withColumn('views', get_list_len_udf('document_id_pv_list')) \\\n",
    "                                 .withColumn('categories', \n",
    "                                             generate_categories_user_profile_map_udf('category_id_lists', \n",
    "                                                                   'cat_confidence_level_lists')) \\\n",
    "                                 .withColumn('topics', \n",
    "                                             generate_topics_user_profile_map_udf('topic_id_lists', \n",
    "                                                                               'top_confidence_level_lists')) \\\n",
    "                                 .withColumn('entities', \n",
    "                                             generate_entities_user_profile_map_udf('entity_id_lists', \n",
    "                                                                               'ent_confidence_level_lists')) \\\n",
    "                                 .select(F.col('uuid_pv').alias('uuid'),\n",
    "                                         F.col('document_id_pv_list').alias('doc_ids'),\n",
    "                                         'views',\n",
    "                                         'categories', 'topics', 'entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spark.executor.cores', '1')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o451.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:213)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 29.0 failed 4 times, most recent failure: Lost task 3.3 in stage 29.0 (TID 2964, cluster-5024-w-1.us-east1-b.c.capstone-project-217608.internal, executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)\n\t... 45 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a3f6e8a11c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtable_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'user_profiles'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0musers_profile_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_BUCKET_FOLDER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o451.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:213)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 29.0 failed 4 times, most recent failure: Lost task 3.3 in stage 29.0 (TID 2964, cluster-5024-w-1.us-east1-b.c.capstone-project-217608.internal, executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)\n\t... 45 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'\n",
    "\n",
    "users_profile_df.write.parquet(OUTPUT_BUCKET_FOLDER+table_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed min:  0.7952461828788121\n"
     ]
    }
   ],
   "source": [
    "finish_time = time.time()\n",
    "print(\"Elapsed min: \", (finish_time-start_time)/60/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
