{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버킷 폴더 생성\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://capstone-01/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://capstone-01/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 불러오기\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 함수 udf\n",
    "# timestamp를 넣으면 일자(숫자) 반환\n",
    "# 예를 들어, events.csv의 timestamp를 이 함수에 넣으면, ad를 클릭한 날이 기준일로부터 몇 일인지 반환. 최소 0일 ~ 최대 12일.\n",
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events.csv 가져오기\n",
    "# ad를 클릭한 날(timestamp 정보)이 첫날로부터 몇 일 경과했는지 기록한 day_event 컬럼 추가\n",
    "# PySpark에서는 파일을 가져올 때 컬럼 type이랑 name 지정해서 가져와야 함\n",
    "# 파일의 schema를 먼저 설정하고, spark.read.schema로 가져옴\n",
    "\n",
    "events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "                .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                .alias('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promoted_content.csv 가져오기\n",
    "\n",
    "promoted_content_schema = StructType(\n",
    "                    [StructField(\"ad_id\", IntegerType(), True),\n",
    "                    StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                    StructField(\"campaign_id\", IntegerType(), True),\n",
    "                    StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                .alias('promoted_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicks_train.csv 가져오기\n",
    "\n",
    "clicks_train_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"ad_id\", IntegerType(), True),                    \n",
    "                    StructField(\"clicked\", IntegerType(), True)]\n",
    "                    )\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"clicks_train.csv\") \\\n",
    "                .alias('clicks_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicks_train_joined.csv 생성하기\n",
    "# clicks_train, promoted_content를 join (key='ad_id')\n",
    "# clicks_train, promoted_content를 events와 join (key='display_id)')\n",
    "# .createOrReplaceTempView : View를 생성하는 함수\n",
    "\n",
    "clicks_train_joined_df = clicks_train_df \\\n",
    "                         .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                         .join(events_df, on='display_id', how='left')                         \n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicks_train_joined에서 display_id, day_event만 select하고 unique한 것만 추출\n",
    "# validation set을 만들기 위해 display_id 샘플링\n",
    "# 샘플링 기준 : day_event(ad를 클릭한 날이 첫날로부터 몇 일 경과했는지)가 0~10이면 0.2 추출, 11~12면 1(전부) 추출\n",
    "# 이 validation set은 clicks_test를 샘플링한 방법과 동일하다\n",
    "# 즉, clicks_test도 day_event가 0~10인 데이터에서 0.2의 비율로, 11~12인 데이터는 전부 추출해서 만들어졌다\n",
    "\n",
    "validation_display_ids_df = clicks_train_joined_df.select('display_id','day_event').distinct() \\\n",
    "                                .sampleBy(\"day_event\", fractions={0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, \\\n",
    "                                                                5: 0.2, 6: 0.2, 7: 0.2, 8: 0.2, 9: 0.2, 10: 0.2, \\\n",
    "                                                               11: 1.0, 12: 1.0}, seed=0)   \n",
    "validation_display_ids_df.createOrReplaceTempView(\"validation_display_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark에서 sql문을 써서 validation_set_df 생성\n",
    "# clicks_train_joined에서 8개 컬럼 select\n",
    "# validation set에 해당하는 display_id만 뽑아 validation_set_df에 저장\n",
    "\n",
    "validation_set_df = spark.sql('''SELECT display_id, ad_id, uuid_event, day_event, timestamp_event,\n",
    "                                        document_id_promo, platform_event, geo_location_event FROM clicks_train_joined t \n",
    "             WHERE EXISTS (SELECT display_id FROM validation_display_ids \n",
    "                           WHERE display_id = t.display_id)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 validation set을 google storage에 저장\n",
    "\n",
    "validation_set_gcs_output = \"validation_set.parquet\"\n",
    "validation_set_df.write.parquet(OUTPUT_BUCKET_FOLDER+validation_set_gcs_output, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark에서 take는 첫 n 개 원소를 뽑아낸 배열을 생성하는 함수. 파이썬의 head 함수와 유사.\n",
    "\n",
    "validation_set_df.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
